{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"relation_extraction_BiGRU_ARTT.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyN8Oiu2N4LOpk9VHoe+zN9V"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"9zag3UWiHbR_","executionInfo":{"status":"ok","timestamp":1659424503386,"user_tz":-480,"elapsed":4414,"user":{"displayName":"Jie Zhang","userId":"17233986917752820527"}}},"outputs":[],"source":["import tensorflow as tf\n","import numpy as np"]},{"cell_type":"code","source":["from keras import Model\n","from keras.layers import Layer\n","import keras.backend as K\n","from keras.layers import Input, Dense, SimpleRNN\n","from sklearn.preprocessing import MinMaxScaler\n","from keras.models import Sequential\n","from keras.metrics import mean_squared_error"],"metadata":{"id":"GFn75TRbjTxS","executionInfo":{"status":"ok","timestamp":1659424503386,"user_tz":-480,"elapsed":4,"user":{"displayName":"Jie Zhang","userId":"17233986917752820527"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')\n","import glob\n","import os\n","import pandas as pd\n","import numpy as np\n","# from googletrans import Translator\n","import re"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-Z84VFo0HnsF","executionInfo":{"status":"ok","timestamp":1659424529271,"user_tz":-480,"elapsed":25888,"user":{"displayName":"Jie Zhang","userId":"17233986917752820527"}},"outputId":"368244ba-e3ba-4edc-8e8a-97cea053cb3a"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["train_word = np.load('/content/drive/MyDrive/Colab Notebooks/brism/relation_extraction_BiGRU_ARTT/data/train_word.npy',allow_pickle=True)"],"metadata":{"id":"ZxnDnZMo6OzI","executionInfo":{"status":"ok","timestamp":1659424531543,"user_tz":-480,"elapsed":2275,"user":{"displayName":"Jie Zhang","userId":"17233986917752820527"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["len(train_word[46])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9-Bfz9sCgir2","executionInfo":{"status":"ok","timestamp":1657261974343,"user_tz":-480,"elapsed":453,"user":{"displayName":"Jie Zhang","userId":"17233986917752820527"}},"outputId":"193eca37-4542-486e-b109-358b68e4fe52"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["2"]},"metadata":{},"execution_count":185}]},{"cell_type":"code","source":["testall_word = np.load('/content/drive/MyDrive/Colab Notebooks/brism/relation_extraction_BiGRU_ARTT/data/testall_word.npy',allow_pickle=True)"],"metadata":{"id":"4zDiF5E0ciWw","executionInfo":{"status":"ok","timestamp":1659424531909,"user_tz":-480,"elapsed":370,"user":{"displayName":"Jie Zhang","userId":"17233986917752820527"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["testall_pos1 = np.load('/content/drive/MyDrive/Colab Notebooks/brism/relation_extraction_BiGRU_ARTT/data/testall_pos1.npy',allow_pickle=True)"],"metadata":{"id":"afxOFYzCcln2","executionInfo":{"status":"ok","timestamp":1659433002039,"user_tz":-480,"elapsed":324,"user":{"displayName":"Jie Zhang","userId":"17233986917752820527"}}},"execution_count":165,"outputs":[]},{"cell_type":"code","source":["testall_pos2 = np.load('/content/drive/MyDrive/Colab Notebooks/brism/relation_extraction_BiGRU_ARTT/data/testall_pos2.npy',allow_pickle=True)"],"metadata":{"id":"W-WGJS6xcoju","executionInfo":{"status":"ok","timestamp":1659433002039,"user_tz":-480,"elapsed":3,"user":{"displayName":"Jie Zhang","userId":"17233986917752820527"}}},"execution_count":166,"outputs":[]},{"cell_type":"code","source":["testall_pos1 = testall_pos1.tolist()\n","testall_pos2 = testall_pos2.tolist()"],"metadata":{"id":"VmzxklJp6g2w","executionInfo":{"status":"ok","timestamp":1659433002039,"user_tz":-480,"elapsed":2,"user":{"displayName":"Jie Zhang","userId":"17233986917752820527"}}},"execution_count":167,"outputs":[]},{"cell_type":"code","source":["testall_y = np.load('/content/drive/MyDrive/Colab Notebooks/brism/relation_extraction_BiGRU_ARTT/data/testall_y.npy',allow_pickle=True)"],"metadata":{"id":"hZcX_A8ObuIH","executionInfo":{"status":"ok","timestamp":1659424532590,"user_tz":-480,"elapsed":6,"user":{"displayName":"Jie Zhang","userId":"17233986917752820527"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["train_x = np.load('/content/drive/MyDrive/Colab Notebooks/brism/relation_extraction_BiGRU_ARTT/data/train_x.npy',allow_pickle=True)"],"metadata":{"id":"1LWnrDLdtyZK","executionInfo":{"status":"ok","timestamp":1659424532940,"user_tz":-480,"elapsed":355,"user":{"displayName":"Jie Zhang","userId":"17233986917752820527"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["train_y = np.load('/content/drive/MyDrive/Colab Notebooks/brism/relation_extraction_BiGRU_ARTT/data/train_y.npy',allow_pickle=True)"],"metadata":{"id":"rTurrDXluKRz","executionInfo":{"status":"ok","timestamp":1659433007784,"user_tz":-480,"elapsed":349,"user":{"displayName":"Jie Zhang","userId":"17233986917752820527"}}},"execution_count":168,"outputs":[]},{"cell_type":"code","source":["#position of word relative to head entity\n","#shape = [sample_size, sent_len]\n","train_pos1 = np.load('/content/drive/MyDrive/Colab Notebooks/brism/relation_extraction_BiGRU_ARTT/data/train_pos1.npy',allow_pickle=True)"],"metadata":{"id":"t1wD1mrZJOdF","executionInfo":{"status":"ok","timestamp":1659424533767,"user_tz":-480,"elapsed":830,"user":{"displayName":"Jie Zhang","userId":"17233986917752820527"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["train_pos2 = np.load('/content/drive/MyDrive/Colab Notebooks/brism/relation_extraction_BiGRU_ARTT/data/train_pos2.npy',allow_pickle=True)"],"metadata":{"id":"WuZrDEeO6MdA","executionInfo":{"status":"ok","timestamp":1659424533768,"user_tz":-480,"elapsed":14,"user":{"displayName":"Jie Zhang","userId":"17233986917752820527"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["len(train_pos1[:50])"],"metadata":{"id":"3kwZ15FWY0YY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# f = open('/content/drive/MyDrive/Colab Notebooks/brism/relation_extraction_BiGRU_ARTT/origin_data/test.txt')    \n","# total_words = 0\n","# i=0\n","# while True:\n","#   content = f.readline()\n","#   if content == '':\n","#     break\n","#   content = content.strip().split()\n","#   total_words += len(content[3])\n","#   i +=1\n","# f.close()\n","# print(i)\n","# # train_pos1 = np.load('/content/drive/MyDrive/Colab Notebooks/brism/relation_extraction_BiGRU_ARTT/data/train_pos1.npy',allow_pickle=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dus0rGBvTRzO","executionInfo":{"status":"ok","timestamp":1657023913709,"user_tz":-480,"elapsed":263,"user":{"displayName":"Jie Zhang","userId":"17233986917752820527"}},"outputId":"5ce0faaf-5e03-4ce2-e699-fe4494e8723d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["100\n"]}]},{"cell_type":"code","source":["test_word = np.load('/content/drive/MyDrive/Colab Notebooks/brism/relation_extraction_BiGRU_ARTT/data/testall_word.npy',allow_pickle=True)"],"metadata":{"id":"WkQrnuLBhDTx","executionInfo":{"status":"ok","timestamp":1659425867827,"user_tz":-480,"elapsed":361,"user":{"displayName":"Jie Zhang","userId":"17233986917752820527"}}},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":["# Replacing 0 in train_word with 123 and 0 in temp_word with 123"],"metadata":{"id":"QnKZyHKz-quE"}},{"cell_type":"code","source":["\n","for i in range(len(train_word)):\n","  if len(train_word[i]) >=2:\n","    for j, item in enumerate(train_word[i]):\n","      if 123 in item:\n","        print(j, i)\n","  else:\n","    if 123 in train_word[i][0]:\n","      print(i)\n"],"metadata":{"id":"TbklWoo9-2lA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# attention_r = tf.random.normal((51,460))\n","# tf.reshape(attention_r, [51*460])\n","# sen_a = tf.random.normal((460,))\n","# sen_r = tf.random.normal((460,1))\n","# relation_embedding = tf.random.normal((12,460))\n","# sen_d = tf.random.normal((12,))\n","# gru_size=460\n","# attention_r[0:1,:]"],"metadata":{"id":"rF3nsTx3hd0r"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_word = np.load('/content/drive/MyDrive/Colab Notebooks/brism/relation_extraction_BiGRU_ARTT/data/train_word.npy',allow_pickle=True)"],"metadata":{"id":"uAqIpit6Jvbt","executionInfo":{"status":"ok","timestamp":1659425873094,"user_tz":-480,"elapsed":546,"user":{"displayName":"Jie Zhang","userId":"17233986917752820527"}}},"execution_count":19,"outputs":[]},{"cell_type":"code","source":["testall_x = np.load('/content/drive/MyDrive/Colab Notebooks/brism/relation_extraction_BiGRU_ARTT/data/testall_x.npy',allow_pickle=True)"],"metadata":{"id":"MG7tFhXNOiea","executionInfo":{"status":"ok","timestamp":1659425873437,"user_tz":-480,"elapsed":2,"user":{"displayName":"Jie Zhang","userId":"17233986917752820527"}}},"execution_count":20,"outputs":[]},{"cell_type":"code","source":["testall_x[0]"],"metadata":{"id":"Q9gQ_CGi861_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_y = np.load('/content/drive/MyDrive/Colab Notebooks/brism/relation_extraction_BiGRU_ARTT/data/train_y.npy',allow_pickle=True)"],"metadata":{"id":"rkXFtGIXOvRs","executionInfo":{"status":"ok","timestamp":1659425876191,"user_tz":-480,"elapsed":525,"user":{"displayName":"Jie Zhang","userId":"17233986917752820527"}}},"execution_count":21,"outputs":[]},{"cell_type":"code","source":["train_y[0]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8MuXfY16o0_o","executionInfo":{"status":"ok","timestamp":1659272288344,"user_tz":-480,"elapsed":405,"user":{"displayName":"Jie Zhang","userId":"17233986917752820527"}},"outputId":"5d91c6d8-e1a7-4be5-9492-b6fef4895495"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0], dtype=object)"]},"metadata":{},"execution_count":138}]},{"cell_type":"code","source":["len(train_x[776])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mldOjMOOP51X","executionInfo":{"status":"ok","timestamp":1657114859424,"user_tz":-480,"elapsed":343,"user":{"displayName":"Jie Zhang","userId":"17233986917752820527"}},"outputId":"1584f77c-ec6f-41b4-fa5b-f92ae1df6ddb"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["2"]},"metadata":{},"execution_count":185}]},{"cell_type":"code","source":["wordembedding = np.load('/content/drive/MyDrive/Colab Notebooks/brism/relation_extraction_BiGRU_ARTT/data/vec.npy',allow_pickle=True)"],"metadata":{"id":"MY3CLp9dO-P8","executionInfo":{"status":"ok","timestamp":1659424535413,"user_tz":-480,"elapsed":349,"user":{"displayName":"Jie Zhang","userId":"17233986917752820527"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["wordembedding_add = np.append(wordembedding,np.array([[0]*100]),axis=0)"],"metadata":{"id":"5rM82u3NPGSz","executionInfo":{"status":"ok","timestamp":1659424535413,"user_tz":-480,"elapsed":3,"user":{"displayName":"Jie Zhang","userId":"17233986917752820527"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","source":["wordembedding_add[-1] = wordembedding_add[123]"],"metadata":{"id":"oZwaqPGWvyaP","executionInfo":{"status":"ok","timestamp":1659424535414,"user_tz":-480,"elapsed":4,"user":{"displayName":"Jie Zhang","userId":"17233986917752820527"}}},"execution_count":16,"outputs":[]},{"cell_type":"code","source":["wordembedding_add[123] = np.array([[123]*100])"],"metadata":{"id":"iWr-xY1C9qq0","executionInfo":{"status":"ok","timestamp":1659424535414,"user_tz":-480,"elapsed":3,"user":{"displayName":"Jie Zhang","userId":"17233986917752820527"}}},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":["# Replacing 123 in train_word with 16117 and 0 in train_pos1/2 with 123"],"metadata":{"id":"Imh3JKbH_nYL"}},{"cell_type":"code","source":["def replace_values(list_to_replace, item_to_replace, item_to_replace_with):\n","    return [item_to_replace_with if item == item_to_replace else item for item in list_to_replace]"],"metadata":{"id":"-2LYEH2I__f_","executionInfo":{"status":"ok","timestamp":1659425879498,"user_tz":-480,"elapsed":534,"user":{"displayName":"Jie Zhang","userId":"17233986917752820527"}}},"execution_count":22,"outputs":[]},{"cell_type":"code","source":["\n","def replace_index(train_word,item_to_replace, item_to_replace_with):\n","  train_word_new = []\n","  for i in range(len(train_word)):\n","    if len(train_word[i])>=2:\n","      temp_list = []\n","      for j in train_word[i]:\n","        new = replace_values(j, item_to_replace, item_to_replace_with)\n","        temp_list.append(new)\n","        # print(new)\n","      train_word_new.append(temp_list)\n","    else:\n","      new = replace_values(train_word[i][0], item_to_replace, item_to_replace_with)\n","      \n","      train_word_new.append([new])\n","  return train_word_new\n"],"metadata":{"id":"VJrgvbrk_09n","executionInfo":{"status":"ok","timestamp":1659425879498,"user_tz":-480,"elapsed":2,"user":{"displayName":"Jie Zhang","userId":"17233986917752820527"}}},"execution_count":23,"outputs":[]},{"cell_type":"code","source":["train_word_new = replace_index(train_word,123, 16117)"],"metadata":{"id":"SDQDqnsWFG74","executionInfo":{"status":"ok","timestamp":1659425887163,"user_tz":-480,"elapsed":321,"user":{"displayName":"Jie Zhang","userId":"17233986917752820527"}}},"execution_count":24,"outputs":[]},{"cell_type":"code","source":["len(train_word_new[46])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-k12hGMCvKA3","executionInfo":{"status":"ok","timestamp":1659430064608,"user_tz":-480,"elapsed":411,"user":{"displayName":"Jie Zhang","userId":"17233986917752820527"}},"outputId":"90fbbeb2-e7bd-459d-df75-f457206eb30a"},"execution_count":114,"outputs":[{"output_type":"execute_result","data":{"text/plain":["2"]},"metadata":{},"execution_count":114}]},{"cell_type":"code","source":["def get_max_sent(train_word_new):\n","  set_lens = []\n","  for i in train_word_new:\n","    set_lens.append(len(i))\n","  return max(set_lens)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kIfK_yU0u9V8","executionInfo":{"status":"ok","timestamp":1659430094404,"user_tz":-480,"elapsed":410,"user":{"displayName":"Jie Zhang","userId":"17233986917752820527"}},"outputId":"0ef68cf2-673e-4370-a21b-ecc64f2702b6"},"execution_count":117,"outputs":[{"output_type":"execute_result","data":{"text/plain":["4"]},"metadata":{},"execution_count":117}]},{"cell_type":"markdown","source":["# Test masking\n"],"metadata":{"id":"lEU8kZvl3_so"}},{"cell_type":"code","source":["x = tf.convert_to_tensor(word_lists)\n","# x =x  tf.Tensor(word_lists, dtype = 'int32',value_index=0)\n","x = tf.reshape(x, [-1,x.shape[2]])\n","mask = tf.not_equal(x, 16117)\n","mask.shape"],"metadata":{"id":"0voCMfMLzo_9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["mask[46]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qgYQyzic1RmO","executionInfo":{"status":"ok","timestamp":1659164337976,"user_tz":-480,"elapsed":279,"user":{"displayName":"Jie Zhang","userId":"17233986917752820527"}},"outputId":"32fbe4a8-b940-4915-b174-4644c2e55a88"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tf.Tensor: shape=(70,), dtype=bool, numpy=\n","array([ True,  True,  True,  True,  True,  True,  True,  True,  True,\n","        True,  True,  True,  True,  True,  True,  True,  True,  True,\n","        True,  True,  True,  True,  True,  True,  True,  True,  True,\n","        True,  True,  True,  True,  True,  True,  True,  True,  True,\n","        True,  True,  True,  True,  True,  True,  True,  True,  True,\n","        True,  True,  True,  True,  True,  True,  True,  True,  True,\n","        True,  True,  True,  True,  True,  True,  True,  True,  True,\n","        True,  True,  True,  True,  True,  True,  True])>"]},"metadata":{},"execution_count":70}]},{"cell_type":"code","source":["from tensorflow import keras"],"metadata":{"id":"JN8_YePNs8vh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from keras.regularizers import Regularizer\n","import keras\n","from tensorflow.python.ops.gen_array_ops import shape\n","from keras.engine import training\n","from keras import layers\n","from tensorflow.python.ops.init_ops_v2 import Initializer\n","class GRU(object):\n","  def __init__(self,word_embeddings,num_steps=70,vocab_size=16118,num_classes=12,gru_size=128,\n","               batch_size=50,pos_num=123,pos_size=5,sent_num = 4):\n","    self.word_embeddings = word_embeddings\n","    # self.total_shape = total_shape\n","    self.num_steps = num_steps\n","    self.vocab_size = vocab_size\n","    self.num_classes = num_classes\n","    self.gru_size = gru_size\n","    self.sent_embed = self.gru_size*2\n","    self.batch_size = batch_size #batch_size = total_num(sent_set的个数),\n","    self.pos_num = pos_num\n","    self.pos_size = pos_size\n","    self.sent_num = sent_num \n","    \n","  def model2(self): #输入的经过处理的flattened sentence\n","    #configure inputs\n","    # total_shape = tf.keras.layers.Input(shape=(self.batch_size+1,),dtype='int32')\n","    # total_shape = tf.keras.layers.Lambda(lambda x: x)(total_shape)\n","    # total_shape = tf.squeeze(total_shape,axis=0)\n","    # print(total_shape[1])\n","    input_word = Input(name = 'input_words', shape=(None,self.num_steps),dtype='int32') #[batch_size, sent_num, sent_len]\n","    input_pos1 = Input(name = 'input_pos1', shape = (None,self.num_steps),dtype='int32') #[batch_size, sent_num, sent_len]\n","    input_pos2 = Input(name = 'input_pos2', shape = (None,self.num_steps),dtype='int32')\n","    input_y = Input(name = 'input_y', shape = (self.num_classes,),dtype='float32')\n","    # print('input_word shape:',input_word.shape)\n","    input_word_reshape = tf.reshape(input_word, [-1,self.sent_num*input_word.shape[2]])\n","    input_pos1_reshape = tf.reshape(input_pos1, [-1,self.sent_num*input_pos1.shape[2]])\n","    input_pos2_reshape = tf.reshape(input_pos2, [-1,self.sent_num*input_pos2.shape[2]])\n","    # total_shape = total_shape[0].numpy().tolist()\n","    # mask = tf.not_equal(input_word_reshape, 16117)\n","    word_embedding = layers.Embedding(self.vocab_size,self.word_embeddings.shape[1],input_length=self.num_steps,\n","                                           embeddings_initializer=tf.initializers.Constant(self.word_embeddings),\n","                                      trainable=False,name = 'word_embedding_matrix')\n","    \n","    pos1_embedding = layers.Embedding(self.pos_num,self.pos_size,name = 'pos1_embedding_matrix') \n","    pos2_embedding = layers.Embedding(self.pos_num,self.pos_size,name = 'pos2_embedding_matrix') \n","    input = word_embedding(input_word_reshape) \n","    pos1 = pos1_embedding(input_pos1_reshape) \n","    pos2 = pos2_embedding(input_pos2_reshape)\n","\n","    input = tf.reshape(input, [-1,self.num_steps,self.word_embeddings.shape[1]]) #[batch_size*sen_num, num_step, embed_size]\n","    pos1 = tf.reshape(pos1, [-1,self.num_steps,self.pos_size])\n","    pos2 = tf.reshape(pos2, [-1,self.num_steps,self.pos_size])\n","   \n","    x = layers.Concatenate(axis=-1)([input,pos1,pos2]) ##[batch_size*sen_num, num_step, embed_size(3)]\n","    masking_layer= layers.Masking(mask_value=123,\n","                                  input_shape=(self.batch_size*self.sent_num, self.num_steps), name= 'masking_layer')(x)\n","    # x = masking_layer(x)\n","    # print(x._keras_mask)\n","    # print(masking_layer[0])\n","    self.gru_layer = layers.Bidirectional(layers.GRU(self.gru_size,return_sequences=True,\n","                                                kernel_regularizer=tf.keras.regularizers.L2(0.001)))(masking_layer) #[batch_size*sen_num, num_step, gru_size*2]\n","    dropout_layer= layers.Dropout(0.5)(self.gru_layer)   \n","    self.dropout_layer = tf.reshape(dropout_layer, [-1, self.sent_num,self.num_steps,self.gru_size*2])     \n","    self.word_attention_layer = Attention1(self.batch_size,self.num_steps,self.sent_num)(self.dropout_layer)\n","    print(\"Mask found:\", self.gru_layer._keras_mask)\n","    # print('word_attention_layer shape:',word_attention_layer.shape) #[flattened_sentence_num,gru_size*2]\n","    outputs = Attention2(self.sent_embed,self.batch_size,self.sent_num,self.num_classes)(self.word_attention_layer)\n","    \n","    # \n","    model = keras.Model(inputs = [input_word,input_pos1,input_pos2], outputs = outputs)\n","    return model\n","class Attention1(Layer): \n","  def __init__(self,batch_size,num_steps,sent_num):\n","    super(Attention1, self).__init__()\n","    self.batch_size = batch_size\n","    self.num_steps = num_steps\n","    self.sent_num = sent_num\n","    self.supports_masking = True\n","\n","  def build(self,input_shape):\n","    #相当于weight initalization\n","    self.attention_w = self.add_weight(name = 'attention_omega', shape = (input_shape[-1],1), initializer=\"random_normal\",\n","                                       trainable=True,regularizer=tf.keras.regularizers.L2(0.001)) #W shape[gru_size,1]\n","    #attention_w用来GRUlayer输出的sentence_embedding（hidden representation of sentences)，提取word_level对于sentence relation\n","    #有重要意义的word，得到的sentence-level feature \n","    # self.sen_a = self.add_weight(name = 'attention_A', shape=(gru_size),trainable=True)\n","    #以上两个用在sentence_set level, 用attention_A提取对于预测sentence的relation有帮助的同样隶属于这个relation的游泳的sentence instances\n","    super(Attention1,self).build(input_shape)\n","  def call(self, x):\n","    # x[batch_size,sen_num, num_step, gru_size*2]\n","    total_sent = self.batch_size*self.sent_num\n","    sent_embed = x.shape[3]\n","    x_reshape = tf.reshape(x, [total_sent, self.num_steps,sent_embed])\n","    # print('x_reshape',x_reshape[0],x_reshape[1],x_reshape[3])\n","    M = tf.reshape(tf.tanh(x), [total_sent*self.num_steps,sent_embed])\n","    \n","    alpha = tf.matmul(M, self.attention_w)\n","    #[total_sent*num_step,gru_size*2][gru_size*2,1] = [total_sent*num_step,1]\n","    alpha = tf.reshape(alpha,[total_sent,self.num_steps]) #[total_sent*num_step,1] - > [total_sent, num_steps]\n","    # print('alpha',alpha[0],alpha[1],alpha[3])\n","    alpha = tf.nn.softmax(alpha,axis=-1) #[total_sent, num_steps]\n","    alpha = tf.reshape(alpha,[total_sent,1,self.num_steps])#[total_sent, num_steps] ->[total_sent,1,num_step] \n","    r = tf.matmul(alpha, x_reshape)#[total_sent,1,num_step][total_sent, num_step,gru_size] = [total_sent,1,gru_size]\n","    sent_embedding = tf.reshape(r,[self.batch_size,self.sent_num,sent_embed]) #[total_sent,1,gru_size*2]-> [batch_size,sen_num,gru_size*2]\n","    \n","    return sent_embedding\n","\n","#sentence-level attention\n","class Attention2(Layer): \n","  def __init__(self,sent_embed,batch_size,sent_num,num_class):\n","    super(Attention2, self).__init__()\n","    self.sent_embed =sent_embed\n","    self.batch_size = batch_size\n","    self.num_class = num_class\n","    self.sent_num = sent_num\n","    self.supports_masking = True\n","  \n","  def build(self,input_shape):\n","    self.sen_a = self.add_weight(name = 'attention_A', shape=(input_shape[-1],),trainable=True,initializer=\"random_normal\",\n","                                       regularizer=tf.keras.regularizers.L2(0.001))\n","                                 \n","                                #  regularizer=tf.keras.regularizers.L2(0.001))\n","    self.sen_r =self.add_weight(name = 'query_r', shape=(input_shape[-1],1),trainable=True,initializer=\"random_normal\",\n","                                       regularizer=tf.keras.regularizers.L2(0.001))\n","                                # regularizer=tf.keras.regularizers.L2(0.001))\n","    self.relation_embedding = self.add_weight(name = 'relation_embedding', shape=(self.num_class,input_shape[-1]),trainable=True,initializer=\"random_normal\",\n","                                       regularizer=tf.keras.regularizers.L2(0.001))\n","                                              # regularizer=tf.keras.regularizers.L2(0.001))\n","    self.sen_d = self.add_weight(name = 'bias', shape=(self.num_class,),initializer=\"zeros\",trainable=True,\n","                                       regularizer=tf.keras.regularizers.L2(0.001))\n","                                #  regularizer=tf.keras.regularizers.L2(0.001))\n","    super(Attention2,self).build(input_shape)\n","  def call(self,x):\n","\n","    #x[batch_size,sen_num,gru_size*2]\n","    sent_repre = [] #list of sentence-set,#每个sent_set将其对应的所有句子的表达提取出来，放进sent_repre list\n","    sent_alpha = [] #list of set-level attention weights\n","    set_repre =[]\n","    relation_score =[]\n","    relation_prob =[]\n","    relation_prediction = []\n","    loss = []\n","    for i in range(self.batch_size): #loop through sent_sets within one batch\n","      # for j in range(self.sent_num):\n","      set_sent = x[i] #[sen_num,gru_size*2]\n","      sent_repre.append(tf.tanh(set_sent))\n","      e = tf.multiply(set_sent,self.sen_a) #[sen_num, gru_size*2][gru_size*2,] = [sen_num, gru_size*2](element-wise multiplication) \n","      e = tf.matmul(e,self.sen_r) #[sen_num, gru_size*2][gru_size*2,1] = [sen_num,1] \n","      e_reshape = tf.reshape(e, [self.sent_num]) #[sen_num,1] -> [sen_num]\n","      set_prob = tf.nn.softmax(e_reshape)  #[sen_num] -> [sen_num]\n","      set_prob_reshape = tf.reshape(set_prob, [1,self.sent_num]) #[sen_num]-> [1,sen_num]\n","      sent_alpha.append(set_prob_reshape)\n","      \n","      s = tf.matmul(sent_alpha[i],sent_repre[i]) #[1,sen_num][sen_num,gru_size*2] = [1,gru_size*2] #整个set的表达（包含sent的不用的重要程度的信息）\n","      #每个句子有不同的weight，#wegihted sum->得到代表着一个set的表达，表示不同的句子/relation对于表达这个set的贡献不同，\n","      #反过来说，这个set包含的relation们的主导程度\n","      #纬度跟单个句子的纬度相同\n","      s_reshape = tf.reshape(s, [self.sent_embed,1])\n","      set_repre.append(s_reshape)\n","      \n","      o=tf.matmul(self.relation_embedding, set_repre[i]) #[num_class, gru_size*2][gru_size*2,1] = [num_class,1]\n","      #这个set属于不同relation的可能性\n","      o_reshape = tf.reshape(o, [self.num_class]) #[num_class,1] -> [num_class]\n","      # print(o_reshape, self.sen_d.shape)\n","      o = tf.add(o_reshape,self.sen_d)\n","      relation_score.append(o) #[batch_size,num_class]\n","      relation_prob.append(tf.nn.softmax(relation_score[i]))\n","      relation_prediction.append(tf.argmax(relation_prob[i],0))\n","    # print('sent_alpha',sent_alpha[0],sent_alpha[1])\n","    return relation_score,relation_prob,relation_prediction\n","  "],"metadata":{"id":"8V9QrguXCYbB","executionInfo":{"status":"ok","timestamp":1659427903356,"user_tz":-480,"elapsed":1383,"user":{"displayName":"Jie Zhang","userId":"17233986917752820527"}}},"execution_count":58,"outputs":[]},{"cell_type":"code","source":["gru = GRU(wordembedding_add,num_steps=70,vocab_size=16118,num_classes=12,gru_size=128,\n","               batch_size=32,pos_num=124,pos_size=6,sent_num=4)\n","model = gru.model2()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"J8tcVrNlFG0K","executionInfo":{"status":"ok","timestamp":1659427910098,"user_tz":-480,"elapsed":3522,"user":{"displayName":"Jie Zhang","userId":"17233986917752820527"}},"outputId":"29e7cb35-fb94-4619-b1d2-4031bdd61086"},"execution_count":59,"outputs":[{"output_type":"stream","name":"stdout","text":["Mask found: KerasTensor(type_spec=TensorSpec(shape=(None, 70), dtype=tf.bool, name=None), name='Placeholder_2:0')\n"]}]},{"cell_type":"code","source":[""],"metadata":{"id":"pZ0FIA9NtiU2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[""],"metadata":{"id":"dhu7o3B7LjLQ"}},{"cell_type":"code","source":["output = model([np.array(word_lists), np.array(pos1_lists), np.array(pos2_lists)])"],"metadata":{"id":"ti2BhNW0OvVr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["output[1][0]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ypvkytZ2zshv","executionInfo":{"status":"ok","timestamp":1659347324622,"user_tz":-480,"elapsed":415,"user":{"displayName":"Jie Zhang","userId":"17233986917752820527"}},"outputId":"94e8e6eb-c4ff-4531-ddd2-08b459a99c58"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tf.Tensor: shape=(12,), dtype=float32, numpy=\n","array([0.11349161, 0.09907483, 0.08728728, 0.08673273, 0.0657768 ,\n","       0.08569241, 0.06923382, 0.10147817, 0.07296032, 0.06079897,\n","       0.06248864, 0.09498443], dtype=float32)>"]},"metadata":{},"execution_count":138}]},{"cell_type":"code","source":["output[2]"],"metadata":{"id":"RfJBXzuczxEn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["output[1][0]\n","tf.argmax(output[1][46],0)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"w8gPlqE2r64N","executionInfo":{"status":"ok","timestamp":1659347382784,"user_tz":-480,"elapsed":438,"user":{"displayName":"Jie Zhang","userId":"17233986917752820527"}},"outputId":"5b7cbc91-d56c-4edf-fc88-6e418477cd65"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tf.Tensor: shape=(), dtype=int64, numpy=0>"]},"metadata":{},"execution_count":141}]},{"cell_type":"code","source":["# class TemporalSoftmax(keras.layers.Layer):\n","#     def call(self, inputs, mask=None):\n","#         broadcast_float_mask = tf.expand_dims(tf.cast(mask, \"float32\"), -1)\n","#         inputs_exp = tf.exp(inputs) * broadcast_float_mask\n","#         inputs_sum = tf.reduce_sum(\n","#             inputs_exp * broadcast_float_mask, axis=-1, keepdims=True\n","#         )\n","#         print(mask)\n","#         return inputs_exp / inputs_sum\n","\n","\n","# inputs = keras.Input(shape=(None,), dtype=\"int32\")\n","# x = layers.Embedding(input_dim=10, output_dim=32, mask_zero=True)(inputs)\n","# x = layers.Dense(1)(x)\n","# outputs = TemporalSoftmax()(x)\n","\n","# model = keras.Model(inputs, outputs)\n","# y = model(np.random.randint(0, 10, size=(32, 100)), np.random.random((32, 100, 1)))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4G9gGuGgPEfk","executionInfo":{"status":"ok","timestamp":1659271887400,"user_tz":-480,"elapsed":399,"user":{"displayName":"Jie Zhang","userId":"17233986917752820527"}},"outputId":"97cf7b31-9f19-43cb-cdd9-45f2938a942f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Tensor(\"Placeholder_1:0\", shape=(None, None), dtype=bool)\n","tf.Tensor(\n","[[ True  True  True ...  True  True  True]\n"," [ True False  True ...  True  True  True]\n"," [ True  True  True ...  True  True  True]\n"," ...\n"," [ True  True  True ...  True  True  True]\n"," [ True False  True ...  True  True  True]\n"," [ True  True  True ...  True  True  True]], shape=(32, 100), dtype=bool)\n"]}]},{"cell_type":"code","source":["model._"],"metadata":{"id":"xyMD0tM7Tp5K"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["output[-1]"],"metadata":{"id":"q1PBxk5GPKLv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# samples, timesteps, features = 32, 10, 8\n","# inputs = np.random.random([samples, timesteps, features]).astype(np.float32)\n","# inputs[:, 3, :] = 0.\n","# inputs[:, 5, :] = 0.\n","\n","# model = tf.keras.models.Sequential()\n","# model.add(tf.keras.layers.Masking(mask_value=0.,\n","#                                   input_shape=(timesteps, features)))\n","# model.add(tf.keras.layers.LSTM(32))\n","\n","# output = model(inputs)"],"metadata":{"id":"c4TZE_RWHJFn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# class CustomEmbedding(keras.layers.Layer):\n","#     def __init__(self, input_dim, output_dim, mask_zero=False, **kwargs):\n","#         super(CustomEmbedding, self).__init__(**kwargs)\n","#         self.input_dim = input_dim\n","#         self.output_dim = output_dim\n","#         self.mask_zero = mask_zero\n","\n","#     def build(self, input_shape):\n","#         self.embeddings = self.add_weight(\n","#             shape=(self.input_dim, self.output_dim),\n","#             initializer=\"random_normal\",\n","#             dtype=\"float32\",\n","#         )\n","\n","#     def call(self, inputs):\n","#         return tf.nn.embedding_lookup(self.embeddings, inputs)\n","\n","#     def compute_mask(self, inputs, mask=None):\n","#         if not self.mask_zero:\n","#             return None\n","#         return tf.not_equal(inputs, 123)\n","\n","\n","# layer = CustomEmbedding(124, 32, mask_zero=True)\n","# x = tf.convert_to_tensor(news, dtype= tf.int32)\n","# y = layer(x)\n","# mask = layer.compute_mask(x)\n","\n","# print(mask)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"j6KFlnzfEyO0","executionInfo":{"status":"ok","timestamp":1659251608889,"user_tz":-480,"elapsed":411,"user":{"displayName":"Jie Zhang","userId":"17233986917752820527"}},"outputId":"30421b18-4711-456b-dfdf-987e0c43b77b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tf.Tensor(\n","[[ True  True  True  True False  True  True  True  True  True]\n"," [ True  True  True  True  True  True  True  True  True False]\n"," [ True  True  True  True False  True  True  True  True  True]], shape=(3, 10), dtype=bool)\n"]}]},{"cell_type":"code","source":["# def calculate_loss(total_num,loss_fn,y):\n","#   for i in total_num:\n","#     loss = loss_fn()"],"metadata":{"id":"H4tsS_K-JqC4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# def custom_loss_function(y_true, y_pred):\n","#    squared_difference = tf.square(y_true - y_pred)\n","#    return tf.reduce_mean(squared_difference, axis=-1)\n","\n","# model.compile(optimizer='adam', loss=custom_loss_function)"],"metadata":{"id":"lr534ctcQyQx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# import numpy as np\n","\n","# y_true = [12, 20, 29., 60.]\n","# y_pred = [14., 18., 27., 55.]\n","# cl = custom_loss_function(np.array(y_true),np.array(y_pred))\n","# cl.numpy()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sWB08FxsQ0Mm","executionInfo":{"status":"ok","timestamp":1657106812391,"user_tz":-480,"elapsed":322,"user":{"displayName":"Jie Zhang","userId":"17233986917752820527"}},"outputId":"9754448d-b55a-4eee-c1b6-ac42f6ffc4e9"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["9.25"]},"metadata":{},"execution_count":159}]},{"cell_type":"code","source":["# optimizer = tf.keras.optimizers.Adam()\n","# optimizer.apply_gradients"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"w4drTyCBVhTO","executionInfo":{"status":"ok","timestamp":1657116592946,"user_tz":-480,"elapsed":399,"user":{"displayName":"Jie Zhang","userId":"17233986917752820527"}},"outputId":"3dd9ed57-c98d-4738-a5c8-b626d2509b90"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<bound method OptimizerV2.apply_gradients of <keras.optimizer_v2.adam.Adam object at 0x7f8ab67fb390>>"]},"metadata":{},"execution_count":192}]},{"cell_type":"code","source":["len(tem_pos1[1]), len(temp_word[1])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KQswjU2S4lzt","executionInfo":{"status":"ok","timestamp":1659432518483,"user_tz":-480,"elapsed":4,"user":{"displayName":"Jie Zhang","userId":"17233986917752820527"}},"outputId":"36244d1b-7138-4bd6-f46d-6a59449521a0"},"execution_count":133,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(1, 1)"]},"metadata":{},"execution_count":133}]},{"cell_type":"code","source":["\n"],"metadata":{"id":"cZoiw90X6Arx","executionInfo":{"status":"ok","timestamp":1659432903924,"user_tz":-480,"elapsed":3,"user":{"displayName":"Jie Zhang","userId":"17233986917752820527"}}},"execution_count":161,"outputs":[]},{"cell_type":"code","source":["[tem_pos1[0]]+[[123]*70]*(4-len(temp_word[0]))"],"metadata":{"id":"akraR6hX4Fwu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["temp_order_train = list(range(len(test_word_new)))\n","np.random.shuffle(temp_order_train)\n","data = []\n","batch_size =32\n","i = 0\n","# for i in range(int(len(temp_order_train) / batch_size)):\n","temp_input = temp_order_train[i * batch_size:(i+1) * batch_size]\n","temp_word = []\n","tem_pos1 = []\n","tem_pos2 = []\n","temp_y = []\n","for k in temp_input:\n","  temp_word.append(train_word[k])\n","  tem_pos1.append(testall_pos1[k])\n","  tem_pos2.append(testall_pos2[k])\n","  temp_y.append(testall_y[k])\n","total_word = [] #total sent (flatten sent_set)\n","total_pos1 = []\n","total_pos2 = []\n","# for i in range(len(temp_word)):\n","#     total_num.append(len(temp_word[i]))\n","word_lists =[]\n","pos1_lists = []\n","pos2_lists = []\n","for id in range(len(temp_word)):\n","  if len(temp_word[id])< 4:\n","    word_lists.append(temp_word[id]+[[123]*70]*(4-len(temp_word[id])))\n","    pos1_lists.append(tem_pos1[id]+[[123]*70]*(4-len(temp_word[id])))\n","    print(pos1_lists)\n","    pos2_lists.append(tem_pos2[id]+[[123]*70]*(4-len(temp_word[id])))\n","  else:\n","    word_lists.append(temp_word[id])\n","    pos1_lists.append(tem_pos1[id])\n","    pos2_lists.append(tem_pos2[id])"],"metadata":{"id":"4tL2CeSI1s3r","executionInfo":{"status":"ok","timestamp":1659432324859,"user_tz":-480,"elapsed":323,"user":{"displayName":"Jie Zhang","userId":"17233986917752820527"}}},"execution_count":121,"outputs":[]},{"cell_type":"code","source":["tem_pos1[0].tolist()+[[123]*70]*(4-len(temp_word[0]))"],"metadata":{"id":"YkW5x03m39lk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"VSmX4xl4DBb8","executionInfo":{"status":"ok","timestamp":1659432935370,"user_tz":-480,"elapsed":3,"user":{"displayName":"Jie Zhang","userId":"17233986917752820527"}}},"execution_count":163,"outputs":[]},{"cell_type":"code","source":["batch_size =32\n","def process_data(train_word,train_pos1,train_pos2,train_y): \n","  temp_order_train = list(range(len(train_word)))\n","  np.random.shuffle(temp_order_train)\n","  data = []\n","  for i in range(int(len(temp_order_train) / batch_size)):\n","    temp_input = temp_order_train[i * batch_size:(i+1) * batch_size]\n","    temp_word = []\n","    tem_pos1 = []\n","    tem_pos2 = []\n","    temp_y = []\n","    for k in temp_input:\n","      temp_word.append(train_word[k])\n","      tem_pos1.append(train_pos1[k])\n","      tem_pos2.append(train_pos2[k])\n","      temp_y.append(train_y[k])\n","    # total_shape = [] #第i个元素表示前i个sent_set一共多少个sentence\n","    # total_num = []\n","    total_word = [] #total sent (flatten sent_set)\n","    total_pos1 = []\n","    total_pos2 = []\n","    # for i in range(len(temp_word)):\n","    #     total_num.append(len(temp_word[i]))\n","    word_lists =[]\n","    pos1_lists = []\n","    pos2_lists = []\n","    for id in range(len(temp_word)):\n","      if len(temp_word[id])< 4:\n","        word_lists.append(temp_word[id]+[[123]*70]*(4-len(temp_word[id])))\n","        pos1_lists.append(tem_pos1[id]+[[123]*70]*(4-len(temp_word[id])))\n","        # print(pos1_lists)\n","        pos2_lists.append(tem_pos2[id]+[[123]*70]*(4-len(temp_word[id])))\n","      else:\n","        word_lists.append(temp_word[id])\n","        pos1_lists.append(tem_pos1[id])\n","        pos2_lists.append(tem_pos2[id])\n","    # break\n","    \n","    word_batch = tf.convert_to_tensor(word_lists)\n","    pos1_batch = tf.convert_to_tensor(pos1_lists)\n","    pos2_batch = tf.convert_to_tensor(pos2_lists)\n","    y_batch = tf.convert_to_tensor(temp_y)\n","    data.append((word_batch,pos1_batch,pos2_batch,y_batch))\n","  \n","  return data"],"metadata":{"id":"SjisAOL_gjiY","executionInfo":{"status":"ok","timestamp":1659433153428,"user_tz":-480,"elapsed":428,"user":{"displayName":"Jie Zhang","userId":"17233986917752820527"}}},"execution_count":178,"outputs":[]},{"cell_type":"code","source":["data = process_data(train_word_new,train_pos1,train_pos2,train_y)"],"metadata":{"id":"S5AW6v1N5AD_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["testall_pos1[0]"],"metadata":{"id":"2lc6oz1f69mT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data = process_data(test_word_new,testall_pos1,testall_pos2,testall_y)"],"metadata":{"id":"wXv3f3o6g0cM","executionInfo":{"status":"ok","timestamp":1659433155987,"user_tz":-480,"elapsed":4,"user":{"displayName":"Jie Zhang","userId":"17233986917752820527"}}},"execution_count":179,"outputs":[]},{"cell_type":"markdown","source":["## Training with fit from scratch"],"metadata":{"id":"FKEHxgtqpslx"}},{"cell_type":"code","source":["test_word_new = replace_index(testall_word,123, 16117)\n","train_word_new = replace_index(train_word,123, 16117)\n","batch_size=32\n","epochs = 20\n","# loss_fn = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n","# optimizer = tf.keras.optimizers.Adam()\n","# acc_metric = keras.metrics.Accuracy()\n","class Training(keras.Model):\n","  def __init__(self,model, batch_size=32, epochs=20,num_class=12,sent_num = 4):\n","    super(Training,self).__init__()\n","    self.model = model\n","    self.batch_size = batch_size\n","    self.epochs = epochs\n","    self.num_class = num_class\n","    self.sent_num =sent_num\n","\n","  def get_databatch(self,train_word,train_pos1,train_pos2,train_y,testall_word,testall_pos1,testall_pos2,testall_y):\n","    self.train_dataset = self.process_data(train_word,train_pos1,train_pos2,train_y)\n","    self.test_dataset = self.process_data(testall_word,testall_pos1,testall_pos2,testall_y)\n","  def process_data(self, train_word,train_pos1,train_pos2,train_y): \n","    temp_order = list(range(len(train_word)))\n","    np.random.shuffle(temp_order)\n","    data = []\n","    for i in range(int(len(temp_order) / self.batch_size)):\n","      temp_input = temp_order[i * self.batch_size:(i+1) * self.batch_size]\n","      temp_word = []\n","      tem_pos1 = []\n","      tem_pos2 = []\n","      temp_y = []\n","      for k in temp_input:\n","        temp_word.append(train_word[k])\n","        tem_pos1.append(train_pos1[k])\n","        tem_pos2.append(train_pos2[k])\n","        temp_y.append(train_y[k])\n","      # total_shape = [] #第i个元素表示前i个sent_set一共多少个sentence\n","      # total_num = []\n","      total_word = [] #total sent (flatten sent_set)\n","      total_pos1 = []\n","      total_pos2 = []\n","      # for i in range(len(temp_word)):\n","      #     total_num.append(len(temp_word[i]))\n","      word_lists =[]\n","      pos1_lists = []\n","      pos2_lists = []\n","      for id in range(len(temp_word)):\n","        if len(temp_word[id])< self.sent_num:\n","          word_lists.append(temp_word[id]+[[123]*70]*(self.sent_num-len(temp_word[id])))\n","          pos1_lists.append(tem_pos1[id]+[[123]*70]*(self.sent_num-len(temp_word[id])))\n","          pos2_lists.append(tem_pos2[id]+[[123]*70]*(self.sent_num-len(temp_word[id])))\n","        else:\n","          word_lists.append(temp_word[id])\n","          pos1_lists.append(tem_pos1[id])\n","          pos2_lists.append(tem_pos2[id])\n","      \n","      word_batch = tf.convert_to_tensor(word_lists)\n","      pos1_batch = tf.convert_to_tensor(pos1_lists)\n","      pos2_batch = tf.convert_to_tensor(pos2_lists)\n","      y_batch = tf.convert_to_tensor(temp_y)\n","      data.append((word_batch,pos1_batch,pos2_batch,y_batch))\n","    \n","    return data\n","  \n","\n","  def train(self):\n","    for one_epoch in range(self.epochs): \n","      print('Epoch {}'.format(one_epoch) +'......')\n","      for i, one_batch_train in enumerate(self.train_dataset):\n","        print('   Training batch {}'.format(i) +':' )\n","        # print('y_batch shape',y_batch.shape)\n","        self.train_step(one_batch_train)\n","      \n","      for j, one_batch_test in enumerate(self.test_dataset):\n","        print('   Test batch {}'.format(j) +':' )\n","        self.test_step(one_batch_test)\n","  def train_step(self, one_batch):\n","    x = one_batch[:3]\n","    y= one_batch[3]\n","    loss_tracker = keras.metrics.CategoricalCrossentropy(name=\"loss\")\n","    accuracy_tracker = keras.metrics.Accuracy(name='accuracy')\n","    loss_fn = keras.losses.CategoricalCrossentropy(from_logits=True)\n","    optimizer = tf.keras.optimizers.Adam()\n","    # # Prepare the metrics.\n","    # train_acc_metric = keras.metrics.SparseCategoricalAccuracy()\n","    # val_acc_metric = keras.metrics.SparseCategoricalAccuracy()\n","    # Iterate over the batches of a dataset.\n","    with tf.GradientTape() as tape:\n","\n","      outputs = self.model(x, training=True) #输入的都是flattened set sentences\n","      logits = tf.convert_to_tensor(outputs[0])\n","      # print('y_array :',y_array,'logits:',logits)\n","      loss = loss_fn(y,logits) \n","      # print('loss:',loss)\n","    preditions= [o.numpy() for o in outputs[2]]\n","    y_pred = tf.one_hot(preditions, self.num_class)\n","    y = y.numpy()\n","    prob = [i.numpy().tolist() for i in outputs[1]]\n","    # print('prob:',prob,'y:',y)\n","    #compute gradients\n","    trainable_vars =self.model.trainable_variables\n","    gradients=tape.gradient(loss,trainable_vars)\n","    #update weights\n","    optimizer.apply_gradients(zip(gradients, trainable_vars))\n","    #update metric\n","    # print('loss_tracker1:',loss_tracker)\n","    loss_tracker.update_state(y,prob)\n","    # print('loss_tracker2:',loss_tracker.result())\n","    accuracy_tracker.update_state(y,y_pred)\n","    print({\"loss\": loss_tracker.result().numpy(), \"accuracy\": accuracy_tracker.result().numpy()})\n","  def test_step(self, one_batch):\n","    x = one_batch[:3]\n","    y= one_batch[3]\n","    accuracy_tracker = keras.metrics.Accuracy(name='accuracy')\n","    # with tf.GradientTape() as tape:\n","    outputs = self.model(x, training=False) #输入的都是flattened set sentences\n","      # loss = loss_fn(y_array,logits)\n","    preditions=  [o.numpy() for o in outputs[2]]\n","    y_pred = tf.one_hot(preditions, self.num_class)\n","    y = y.numpy()\n","    accuracy_tracker.update_state(y_pred,y)\n","    print({\"accuracy\": accuracy_tracker.result().numpy()})\n","\n"],"metadata":{"id":"YZDJwoDyYwNl","executionInfo":{"status":"ok","timestamp":1659434720681,"user_tz":-480,"elapsed":549,"user":{"displayName":"Jie Zhang","userId":"17233986917752820527"}}},"execution_count":204,"outputs":[]},{"cell_type":"code","source":["training = Training(model,batch_size=32, epochs=20,num_class =12,sent_num = 4)"],"metadata":{"id":"-IPKwgRTdZrI","executionInfo":{"status":"ok","timestamp":1659434721796,"user_tz":-480,"elapsed":3,"user":{"displayName":"Jie Zhang","userId":"17233986917752820527"}}},"execution_count":205,"outputs":[]},{"cell_type":"code","source":["training.get_databatch(train_word_new,train_pos1,train_pos2,train_y,\n","                       test_word_new,testall_pos1,testall_pos2,testall_y)"],"metadata":{"id":"J_shBUnsoW74","executionInfo":{"status":"ok","timestamp":1659434724422,"user_tz":-480,"elapsed":547,"user":{"displayName":"Jie Zhang","userId":"17233986917752820527"}}},"execution_count":206,"outputs":[]},{"cell_type":"code","source":["training.train()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"M_pdvRQ07btz","executionInfo":{"status":"ok","timestamp":1659435613563,"user_tz":-480,"elapsed":886603,"user":{"displayName":"Jie Zhang","userId":"17233986917752820527"}},"outputId":"7dde82d8-71ce-4f6e-94ab-10bc849be70d"},"execution_count":207,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 0......\n","   Training batch 0:\n","ERROR:tensorflow:==================================\n","Object was never used (type <class 'tensorflow.python.ops.tensor_array_ops.TensorArray'>):\n","<tensorflow.python.ops.tensor_array_ops.TensorArray object at 0x7f4ac722bb50>\n","If you want to mark it as used call its \"mark_used()\" method.\n","It was originally created here:\n","  File \"/usr/local/lib/python3.7/dist-packages/keras/backend.py\", line 4705, in <genexpr>\n","    for ta, out in zip(output_ta_t, flat_new_output))  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/tf_should_use.py\", line 245, in wrapped\n","    error_in_function=error_in_function)\n","==================================\n"]},{"output_type":"stream","name":"stderr","text":["ERROR:tensorflow:==================================\n","Object was never used (type <class 'tensorflow.python.ops.tensor_array_ops.TensorArray'>):\n","<tensorflow.python.ops.tensor_array_ops.TensorArray object at 0x7f4ac722bb50>\n","If you want to mark it as used call its \"mark_used()\" method.\n","It was originally created here:\n","  File \"/usr/local/lib/python3.7/dist-packages/keras/backend.py\", line 4705, in <genexpr>\n","    for ta, out in zip(output_ta_t, flat_new_output))  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/tf_should_use.py\", line 245, in wrapped\n","    error_in_function=error_in_function)\n","==================================\n"]},{"output_type":"stream","name":"stdout","text":["ERROR:tensorflow:==================================\n","Object was never used (type <class 'tensorflow.python.ops.tensor_array_ops.TensorArray'>):\n","<tensorflow.python.ops.tensor_array_ops.TensorArray object at 0x7f4ac73f0f50>\n","If you want to mark it as used call its \"mark_used()\" method.\n","It was originally created here:\n","  File \"/usr/local/lib/python3.7/dist-packages/keras/backend.py\", line 4705, in <genexpr>\n","    for ta, out in zip(output_ta_t, flat_new_output))  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/tf_should_use.py\", line 245, in wrapped\n","    error_in_function=error_in_function)\n","==================================\n"]},{"output_type":"stream","name":"stderr","text":["ERROR:tensorflow:==================================\n","Object was never used (type <class 'tensorflow.python.ops.tensor_array_ops.TensorArray'>):\n","<tensorflow.python.ops.tensor_array_ops.TensorArray object at 0x7f4ac73f0f50>\n","If you want to mark it as used call its \"mark_used()\" method.\n","It was originally created here:\n","  File \"/usr/local/lib/python3.7/dist-packages/keras/backend.py\", line 4705, in <genexpr>\n","    for ta, out in zip(output_ta_t, flat_new_output))  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/tf_should_use.py\", line 245, in wrapped\n","    error_in_function=error_in_function)\n","==================================\n"]},{"output_type":"stream","name":"stdout","text":["{'loss': 1.5195329, 'accuracy': 0.921875}\n","   Training batch 1:\n","{'loss': 1.7633479, 'accuracy': 0.90625}\n","   Training batch 2:\n","{'loss': 1.5448565, 'accuracy': 0.9166667}\n","   Training batch 3:\n","{'loss': 1.6985173, 'accuracy': 0.9270833}\n","   Training batch 4:\n","{'loss': 1.8634574, 'accuracy': 0.9010417}\n","   Training batch 5:\n","{'loss': 1.6561835, 'accuracy': 0.9114583}\n","   Training batch 6:\n","{'loss': 1.6949928, 'accuracy': 0.921875}\n","   Training batch 7:\n","{'loss': 2.0039253, 'accuracy': 0.875}\n","   Training batch 8:\n","{'loss': 1.2872593, 'accuracy': 0.9270833}\n","   Training batch 9:\n","{'loss': 1.646263, 'accuracy': 0.921875}\n","   Training batch 10:\n","{'loss': 1.7782829, 'accuracy': 0.9010417}\n","   Training batch 11:\n","{'loss': 1.7266119, 'accuracy': 0.9114583}\n","   Training batch 12:\n","{'loss': 1.833074, 'accuracy': 0.9114583}\n","   Training batch 13:\n","{'loss': 1.8117883, 'accuracy': 0.921875}\n","   Training batch 14:\n","{'loss': 2.3699613, 'accuracy': 0.8802083}\n","   Training batch 15:\n","{'loss': 1.859924, 'accuracy': 0.9010417}\n","   Training batch 16:\n","{'loss': 1.7121127, 'accuracy': 0.9114583}\n","   Training batch 17:\n","{'loss': 1.6631336, 'accuracy': 0.9114583}\n","   Training batch 18:\n","{'loss': 1.7723546, 'accuracy': 0.90625}\n","   Training batch 19:\n","{'loss': 1.6328294, 'accuracy': 0.9322917}\n","   Training batch 20:\n","{'loss': 1.8469148, 'accuracy': 0.90625}\n","   Training batch 21:\n","{'loss': 1.6517041, 'accuracy': 0.9010417}\n","   Training batch 22:\n","{'loss': 1.724442, 'accuracy': 0.8958333}\n","   Training batch 23:\n","{'loss': 1.5898433, 'accuracy': 0.9322917}\n","   Training batch 24:\n","{'loss': 2.0488725, 'accuracy': 0.8958333}\n","   Training batch 25:\n","{'loss': 1.4346832, 'accuracy': 0.9166667}\n","   Training batch 26:\n","{'loss': 1.7298372, 'accuracy': 0.9010417}\n","   Training batch 27:\n","{'loss': 1.7842312, 'accuracy': 0.90625}\n","   Training batch 28:\n","{'loss': 1.846004, 'accuracy': 0.9114583}\n","   Training batch 29:\n","{'loss': 1.4970305, 'accuracy': 0.9114583}\n","   Test batch 0:\n","{'accuracy': 0.8958333}\n","   Test batch 1:\n","{'accuracy': 0.9166667}\n","   Test batch 2:\n","{'accuracy': 0.9166667}\n","Epoch 1......\n","   Training batch 0:\n","{'loss': 1.5372285, 'accuracy': 0.9114583}\n","   Training batch 1:\n","{'loss': 1.7290063, 'accuracy': 0.90625}\n","   Training batch 2:\n","{'loss': 1.5479133, 'accuracy': 0.9322917}\n","   Training batch 3:\n","{'loss': 1.7219076, 'accuracy': 0.9270833}\n","   Training batch 4:\n","{'loss': 1.7090755, 'accuracy': 0.90625}\n","   Training batch 5:\n","{'loss': 1.665559, 'accuracy': 0.9114583}\n","   Training batch 6:\n","{'loss': 1.6879644, 'accuracy': 0.9166667}\n","   Training batch 7:\n","{'loss': 2.0118964, 'accuracy': 0.8697917}\n","   Training batch 8:\n","{'loss': 1.2930754, 'accuracy': 0.9322917}\n","   Training batch 9:\n","{'loss': 1.5958248, 'accuracy': 0.921875}\n","   Training batch 10:\n","{'loss': 1.7232944, 'accuracy': 0.9010417}\n","   Training batch 11:\n","{'loss': 1.683436, 'accuracy': 0.9114583}\n","   Training batch 12:\n","{'loss': 1.8693181, 'accuracy': 0.8854167}\n","   Training batch 13:\n","{'loss': 1.8139558, 'accuracy': 0.9270833}\n","   Training batch 14:\n","{'loss': 2.3317742, 'accuracy': 0.8802083}\n","   Training batch 15:\n","{'loss': 1.8557866, 'accuracy': 0.9010417}\n","   Training batch 16:\n","{'loss': 1.6461202, 'accuracy': 0.90625}\n","   Training batch 17:\n","{'loss': 1.5921108, 'accuracy': 0.9114583}\n","   Training batch 18:\n","{'loss': 1.7910919, 'accuracy': 0.8958333}\n","   Training batch 19:\n","{'loss': 1.6444709, 'accuracy': 0.9375}\n","   Training batch 20:\n","{'loss': 1.8555202, 'accuracy': 0.9010417}\n","   Training batch 21:\n","{'loss': 1.6068382, 'accuracy': 0.8958333}\n","   Training batch 22:\n","{'loss': 1.7314699, 'accuracy': 0.9010417}\n","   Training batch 23:\n","{'loss': 1.6239626, 'accuracy': 0.9375}\n","   Training batch 24:\n","{'loss': 2.0124948, 'accuracy': 0.8958333}\n","   Training batch 25:\n","{'loss': 1.3956275, 'accuracy': 0.9166667}\n","   Training batch 26:\n","{'loss': 1.7784114, 'accuracy': 0.8958333}\n","   Training batch 27:\n","{'loss': 1.7755218, 'accuracy': 0.90625}\n","   Training batch 28:\n","{'loss': 1.7955024, 'accuracy': 0.90625}\n","   Training batch 29:\n","{'loss': 1.4801278, 'accuracy': 0.9114583}\n","   Test batch 0:\n","{'accuracy': 0.9114583}\n","   Test batch 1:\n","{'accuracy': 0.9322917}\n","   Test batch 2:\n","{'accuracy': 0.9166667}\n","Epoch 2......\n","   Training batch 0:\n","{'loss': 1.4803804, 'accuracy': 0.921875}\n","   Training batch 1:\n","{'loss': 1.7959878, 'accuracy': 0.9010417}\n","   Training batch 2:\n","{'loss': 1.5561092, 'accuracy': 0.9270833}\n","   Training batch 3:\n","{'loss': 1.7432643, 'accuracy': 0.9270833}\n","   Training batch 4:\n","{'loss': 1.6859032, 'accuracy': 0.9114583}\n","   Training batch 5:\n","{'loss': 1.62025, 'accuracy': 0.9166667}\n","   Training batch 6:\n","{'loss': 1.6371717, 'accuracy': 0.9166667}\n","   Training batch 7:\n","{'loss': 2.0294065, 'accuracy': 0.8697917}\n","   Training batch 8:\n","{'loss': 1.2690562, 'accuracy': 0.9322917}\n","   Training batch 9:\n","{'loss': 1.5927579, 'accuracy': 0.921875}\n","   Training batch 10:\n","{'loss': 1.7222095, 'accuracy': 0.90625}\n","   Training batch 11:\n","{'loss': 1.6247323, 'accuracy': 0.9114583}\n","   Training batch 12:\n","{'loss': 1.7732596, 'accuracy': 0.9114583}\n","   Training batch 13:\n","{'loss': 1.73353, 'accuracy': 0.9270833}\n","   Training batch 14:\n","{'loss': 2.3706424, 'accuracy': 0.8802083}\n","   Training batch 15:\n","{'loss': 1.8378011, 'accuracy': 0.9114583}\n","   Training batch 16:\n","{'loss': 1.631304, 'accuracy': 0.90625}\n","   Training batch 17:\n","{'loss': 1.5244886, 'accuracy': 0.9114583}\n","   Training batch 18:\n","{'loss': 1.7995754, 'accuracy': 0.8958333}\n","   Training batch 19:\n","{'loss': 1.5981126, 'accuracy': 0.9375}\n","   Training batch 20:\n","{'loss': 1.880209, 'accuracy': 0.9010417}\n","   Training batch 21:\n","{'loss': 1.6285902, 'accuracy': 0.90625}\n","   Training batch 22:\n","{'loss': 1.7441554, 'accuracy': 0.90625}\n","   Training batch 23:\n","{'loss': 1.5951352, 'accuracy': 0.9375}\n","   Training batch 24:\n","{'loss': 1.9379312, 'accuracy': 0.8958333}\n","   Training batch 25:\n","{'loss': 1.3899186, 'accuracy': 0.9166667}\n","   Training batch 26:\n","{'loss': 1.80001, 'accuracy': 0.8958333}\n","   Training batch 27:\n","{'loss': 1.742797, 'accuracy': 0.9114583}\n","   Training batch 28:\n","{'loss': 1.7588551, 'accuracy': 0.90625}\n","   Training batch 29:\n","{'loss': 1.4859974, 'accuracy': 0.9166667}\n","   Test batch 0:\n","{'accuracy': 0.90625}\n","   Test batch 1:\n","{'accuracy': 0.9322917}\n","   Test batch 2:\n","{'accuracy': 0.921875}\n","Epoch 3......\n","   Training batch 0:\n","{'loss': 1.419029, 'accuracy': 0.9322917}\n","   Training batch 1:\n","{'loss': 1.7945453, 'accuracy': 0.90625}\n","   Training batch 2:\n","{'loss': 1.541721, 'accuracy': 0.9322917}\n","   Training batch 3:\n","{'loss': 1.6834095, 'accuracy': 0.9270833}\n","   Training batch 4:\n","{'loss': 1.7166021, 'accuracy': 0.90625}\n","   Training batch 5:\n","{'loss': 1.5832949, 'accuracy': 0.9166667}\n","   Training batch 6:\n","{'loss': 1.5920925, 'accuracy': 0.9166667}\n","   Training batch 7:\n","{'loss': 2.0168796, 'accuracy': 0.875}\n","   Training batch 8:\n","{'loss': 1.2592239, 'accuracy': 0.9375}\n","   Training batch 9:\n","{'loss': 1.5863287, 'accuracy': 0.9270833}\n","   Training batch 10:\n","{'loss': 1.6873668, 'accuracy': 0.9114583}\n","   Training batch 11:\n","{'loss': 1.5921793, 'accuracy': 0.9166667}\n","   Training batch 12:\n","{'loss': 1.7457787, 'accuracy': 0.90625}\n","   Training batch 13:\n","{'loss': 1.6884966, 'accuracy': 0.9270833}\n","   Training batch 14:\n","{'loss': 2.2997608, 'accuracy': 0.8802083}\n","   Training batch 15:\n","{'loss': 1.82226, 'accuracy': 0.9114583}\n","   Training batch 16:\n","{'loss': 1.5597728, 'accuracy': 0.90625}\n","   Training batch 17:\n","{'loss': 1.5269861, 'accuracy': 0.9010417}\n","   Training batch 18:\n","{'loss': 1.7810165, 'accuracy': 0.9010417}\n","   Training batch 19:\n","{'loss': 1.5714324, 'accuracy': 0.9322917}\n","   Training batch 20:\n","{'loss': 1.8334973, 'accuracy': 0.90625}\n","   Training batch 21:\n","{'loss': 1.6227372, 'accuracy': 0.9114583}\n","   Training batch 22:\n","{'loss': 1.7215111, 'accuracy': 0.9010417}\n","   Training batch 23:\n","{'loss': 1.5580232, 'accuracy': 0.9270833}\n","   Training batch 24:\n","{'loss': 1.9214989, 'accuracy': 0.8958333}\n","   Training batch 25:\n","{'loss': 1.3826244, 'accuracy': 0.9166667}\n","   Training batch 26:\n","{'loss': 1.7658558, 'accuracy': 0.9010417}\n","   Training batch 27:\n","{'loss': 1.7325671, 'accuracy': 0.9114583}\n","   Training batch 28:\n","{'loss': 1.7399838, 'accuracy': 0.90625}\n","   Training batch 29:\n","{'loss': 1.4627144, 'accuracy': 0.9166667}\n","   Test batch 0:\n","{'accuracy': 0.9114583}\n","   Test batch 1:\n","{'accuracy': 0.9322917}\n","   Test batch 2:\n","{'accuracy': 0.921875}\n","Epoch 4......\n","   Training batch 0:\n","{'loss': 1.3615253, 'accuracy': 0.9322917}\n","   Training batch 1:\n","{'loss': 1.7636943, 'accuracy': 0.90625}\n","   Training batch 2:\n","{'loss': 1.5585675, 'accuracy': 0.9322917}\n","   Training batch 3:\n","{'loss': 1.6528109, 'accuracy': 0.9322917}\n","   Training batch 4:\n","{'loss': 1.7473605, 'accuracy': 0.90625}\n","   Training batch 5:\n","{'loss': 1.5699928, 'accuracy': 0.921875}\n","   Training batch 6:\n","{'loss': 1.5889776, 'accuracy': 0.9270833}\n","   Training batch 7:\n","{'loss': 1.9817967, 'accuracy': 0.8802083}\n","   Training batch 8:\n","{'loss': 1.2235773, 'accuracy': 0.9375}\n","   Training batch 9:\n","{'loss': 1.5462132, 'accuracy': 0.9270833}\n","   Training batch 10:\n","{'loss': 1.6346581, 'accuracy': 0.9114583}\n","   Training batch 11:\n","{'loss': 1.6010418, 'accuracy': 0.9166667}\n","   Training batch 12:\n","{'loss': 1.7123468, 'accuracy': 0.90625}\n","   Training batch 13:\n","{'loss': 1.6267059, 'accuracy': 0.9270833}\n","   Training batch 14:\n","{'loss': 2.3298802, 'accuracy': 0.8802083}\n","   Training batch 15:\n","{'loss': 1.8067158, 'accuracy': 0.9114583}\n","   Training batch 16:\n","{'loss': 1.5040501, 'accuracy': 0.9114583}\n","   Training batch 17:\n","{'loss': 1.5203855, 'accuracy': 0.9114583}\n","   Training batch 18:\n","{'loss': 1.7464453, 'accuracy': 0.9114583}\n","   Training batch 19:\n","{'loss': 1.5030105, 'accuracy': 0.9427083}\n","   Training batch 20:\n","{'loss': 1.9350872, 'accuracy': 0.8958333}\n","   Training batch 21:\n","{'loss': 1.572051, 'accuracy': 0.9114583}\n","   Training batch 22:\n","{'loss': 1.6416839, 'accuracy': 0.90625}\n","   Training batch 23:\n","{'loss': 1.5116776, 'accuracy': 0.9270833}\n","   Training batch 24:\n","{'loss': 1.8940585, 'accuracy': 0.8958333}\n","   Training batch 25:\n","{'loss': 1.3912255, 'accuracy': 0.921875}\n","   Training batch 26:\n","{'loss': 1.7546403, 'accuracy': 0.9010417}\n","   Training batch 27:\n","{'loss': 1.7258527, 'accuracy': 0.90625}\n","   Training batch 28:\n","{'loss': 1.7075539, 'accuracy': 0.9166667}\n","   Training batch 29:\n","{'loss': 1.4103184, 'accuracy': 0.9166667}\n","   Test batch 0:\n","{'accuracy': 0.9166667}\n","   Test batch 1:\n","{'accuracy': 0.921875}\n","   Test batch 2:\n","{'accuracy': 0.921875}\n","Epoch 5......\n","   Training batch 0:\n","{'loss': 1.32567, 'accuracy': 0.9270833}\n","   Training batch 1:\n","{'loss': 1.8181566, 'accuracy': 0.90625}\n","   Training batch 2:\n","{'loss': 1.496016, 'accuracy': 0.9270833}\n","   Training batch 3:\n","{'loss': 1.6168293, 'accuracy': 0.9322917}\n","   Training batch 4:\n","{'loss': 1.7035407, 'accuracy': 0.90625}\n","   Training batch 5:\n","{'loss': 1.4908429, 'accuracy': 0.9270833}\n","   Training batch 6:\n","{'loss': 1.5140657, 'accuracy': 0.9270833}\n","   Training batch 7:\n","{'loss': 2.0703127, 'accuracy': 0.875}\n","   Training batch 8:\n","{'loss': 1.231046, 'accuracy': 0.9375}\n","   Training batch 9:\n","{'loss': 1.4960008, 'accuracy': 0.9322917}\n","   Training batch 10:\n","{'loss': 1.6188179, 'accuracy': 0.9114583}\n","   Training batch 11:\n","{'loss': 1.56661, 'accuracy': 0.9114583}\n","   Training batch 12:\n","{'loss': 1.6809405, 'accuracy': 0.90625}\n","   Training batch 13:\n","{'loss': 1.5891517, 'accuracy': 0.9270833}\n","   Training batch 14:\n","{'loss': 2.2666311, 'accuracy': 0.8802083}\n","   Training batch 15:\n","{'loss': 1.72736, 'accuracy': 0.9114583}\n","   Training batch 16:\n","{'loss': 1.4430869, 'accuracy': 0.9114583}\n","   Training batch 17:\n","{'loss': 1.4492003, 'accuracy': 0.90625}\n","   Training batch 18:\n","{'loss': 1.7559226, 'accuracy': 0.8958333}\n","   Training batch 19:\n","{'loss': 1.4382133, 'accuracy': 0.9375}\n","   Training batch 20:\n","{'loss': 1.914262, 'accuracy': 0.90625}\n","   Training batch 21:\n","{'loss': 1.543451, 'accuracy': 0.921875}\n","   Training batch 22:\n","{'loss': 1.6147544, 'accuracy': 0.90625}\n","   Training batch 23:\n","{'loss': 1.5044326, 'accuracy': 0.9270833}\n","   Training batch 24:\n","{'loss': 1.8619521, 'accuracy': 0.8958333}\n","   Training batch 25:\n","{'loss': 1.3442318, 'accuracy': 0.921875}\n","   Training batch 26:\n","{'loss': 1.6671747, 'accuracy': 0.9010417}\n","   Training batch 27:\n","{'loss': 1.7058449, 'accuracy': 0.90625}\n","   Training batch 28:\n","{'loss': 1.6936556, 'accuracy': 0.9114583}\n","   Training batch 29:\n","{'loss': 1.3790572, 'accuracy': 0.921875}\n","   Test batch 0:\n","{'accuracy': 0.9114583}\n","   Test batch 1:\n","{'accuracy': 0.9166667}\n","   Test batch 2:\n","{'accuracy': 0.921875}\n","Epoch 6......\n","   Training batch 0:\n","{'loss': 1.2846137, 'accuracy': 0.9270833}\n","   Training batch 1:\n","{'loss': 1.7792015, 'accuracy': 0.9166667}\n","   Training batch 2:\n","{'loss': 1.4379563, 'accuracy': 0.9375}\n","   Training batch 3:\n","{'loss': 1.5704283, 'accuracy': 0.9270833}\n","   Training batch 4:\n","{'loss': 1.6673398, 'accuracy': 0.9114583}\n","   Training batch 5:\n","{'loss': 1.4274025, 'accuracy': 0.9270833}\n","   Training batch 6:\n","{'loss': 1.4725815, 'accuracy': 0.9322917}\n","   Training batch 7:\n","{'loss': 2.1224928, 'accuracy': 0.875}\n","   Training batch 8:\n","{'loss': 1.2195415, 'accuracy': 0.9375}\n","   Training batch 9:\n","{'loss': 1.4279635, 'accuracy': 0.9375}\n","   Training batch 10:\n","{'loss': 1.5685852, 'accuracy': 0.9114583}\n","   Training batch 11:\n","{'loss': 1.5653276, 'accuracy': 0.9114583}\n","   Training batch 12:\n","{'loss': 1.6687199, 'accuracy': 0.90625}\n","   Training batch 13:\n","{'loss': 1.5577698, 'accuracy': 0.9322917}\n","   Training batch 14:\n","{'loss': 2.1170769, 'accuracy': 0.8802083}\n","   Training batch 15:\n","{'loss': 1.6724063, 'accuracy': 0.90625}\n","   Training batch 16:\n","{'loss': 1.4380301, 'accuracy': 0.9166667}\n","   Training batch 17:\n","{'loss': 1.4298413, 'accuracy': 0.9166667}\n","   Training batch 18:\n","{'loss': 1.7000835, 'accuracy': 0.8958333}\n","   Training batch 19:\n","{'loss': 1.3788644, 'accuracy': 0.9375}\n","   Training batch 20:\n","{'loss': 1.9384267, 'accuracy': 0.9010417}\n","   Training batch 21:\n","{'loss': 1.5026207, 'accuracy': 0.9166667}\n","   Training batch 22:\n","{'loss': 1.5881748, 'accuracy': 0.90625}\n","   Training batch 23:\n","{'loss': 1.4738973, 'accuracy': 0.9270833}\n","   Training batch 24:\n","{'loss': 1.8732109, 'accuracy': 0.890625}\n","   Training batch 25:\n","{'loss': 1.3362087, 'accuracy': 0.9166667}\n","   Training batch 26:\n","{'loss': 1.6953586, 'accuracy': 0.90625}\n","   Training batch 27:\n","{'loss': 1.7338816, 'accuracy': 0.90625}\n","   Training batch 28:\n","{'loss': 1.6589091, 'accuracy': 0.9114583}\n","   Training batch 29:\n","{'loss': 1.3677311, 'accuracy': 0.921875}\n","   Test batch 0:\n","{'accuracy': 0.9166667}\n","   Test batch 1:\n","{'accuracy': 0.921875}\n","   Test batch 2:\n","{'accuracy': 0.9270833}\n","Epoch 7......\n","   Training batch 0:\n","{'loss': 1.2312591, 'accuracy': 0.9427083}\n","   Training batch 1:\n","{'loss': 1.7595679, 'accuracy': 0.9166667}\n","   Training batch 2:\n","{'loss': 1.499187, 'accuracy': 0.9270833}\n","   Training batch 3:\n","{'loss': 1.5428137, 'accuracy': 0.9322917}\n","   Training batch 4:\n","{'loss': 1.6676493, 'accuracy': 0.9114583}\n","   Training batch 5:\n","{'loss': 1.4056296, 'accuracy': 0.9270833}\n","   Training batch 6:\n","{'loss': 1.5242234, 'accuracy': 0.921875}\n","   Training batch 7:\n","{'loss': 1.9989853, 'accuracy': 0.875}\n","   Training batch 8:\n","{'loss': 1.1619782, 'accuracy': 0.9375}\n","   Training batch 9:\n","{'loss': 1.3920512, 'accuracy': 0.9322917}\n","   Training batch 10:\n","{'loss': 1.5583428, 'accuracy': 0.90625}\n","   Training batch 11:\n","{'loss': 1.6065959, 'accuracy': 0.9010417}\n","   Training batch 12:\n","{'loss': 1.6261566, 'accuracy': 0.90625}\n","   Training batch 13:\n","{'loss': 1.5349066, 'accuracy': 0.9270833}\n","   Training batch 14:\n","{'loss': 2.0605564, 'accuracy': 0.890625}\n","   Training batch 15:\n","{'loss': 1.6328449, 'accuracy': 0.921875}\n","   Training batch 16:\n","{'loss': 1.416085, 'accuracy': 0.9114583}\n","   Training batch 17:\n","{'loss': 1.3870215, 'accuracy': 0.9270833}\n","   Training batch 18:\n","{'loss': 1.7438072, 'accuracy': 0.890625}\n","   Training batch 19:\n","{'loss': 1.4869546, 'accuracy': 0.9270833}\n","   Training batch 20:\n","{'loss': 1.9646815, 'accuracy': 0.90625}\n","   Training batch 21:\n","{'loss': 1.4404287, 'accuracy': 0.9270833}\n","   Training batch 22:\n","{'loss': 1.5864568, 'accuracy': 0.90625}\n","   Training batch 23:\n","{'loss': 1.5104709, 'accuracy': 0.9322917}\n","   Training batch 24:\n","{'loss': 1.8378284, 'accuracy': 0.8958333}\n","   Training batch 25:\n","{'loss': 1.3656299, 'accuracy': 0.9114583}\n","   Training batch 26:\n","{'loss': 1.7052782, 'accuracy': 0.90625}\n","   Training batch 27:\n","{'loss': 1.7676566, 'accuracy': 0.90625}\n","   Training batch 28:\n","{'loss': 1.6298015, 'accuracy': 0.921875}\n","   Training batch 29:\n","{'loss': 1.3319306, 'accuracy': 0.921875}\n","   Test batch 0:\n","{'accuracy': 0.9114583}\n","   Test batch 1:\n","{'accuracy': 0.9270833}\n","   Test batch 2:\n","{'accuracy': 0.9270833}\n","Epoch 8......\n","   Training batch 0:\n","{'loss': 1.2010753, 'accuracy': 0.9479167}\n","   Training batch 1:\n","{'loss': 1.8808389, 'accuracy': 0.90625}\n","   Training batch 2:\n","{'loss': 1.4087663, 'accuracy': 0.9375}\n","   Training batch 3:\n","{'loss': 1.4798791, 'accuracy': 0.9270833}\n","   Training batch 4:\n","{'loss': 1.7699294, 'accuracy': 0.9010417}\n","   Training batch 5:\n","{'loss': 1.3681657, 'accuracy': 0.9270833}\n","   Training batch 6:\n","{'loss': 1.4218574, 'accuracy': 0.9375}\n","   Training batch 7:\n","{'loss': 2.107996, 'accuracy': 0.875}\n","   Training batch 8:\n","{'loss': 1.1847993, 'accuracy': 0.9375}\n","   Training batch 9:\n","{'loss': 1.3729097, 'accuracy': 0.9375}\n","   Training batch 10:\n","{'loss': 1.5429502, 'accuracy': 0.9114583}\n","   Training batch 11:\n","{'loss': 1.5880795, 'accuracy': 0.921875}\n","   Training batch 12:\n","{'loss': 1.6781356, 'accuracy': 0.9010417}\n","   Training batch 13:\n","{'loss': 1.4906414, 'accuracy': 0.9322917}\n","   Training batch 14:\n","{'loss': 2.0041656, 'accuracy': 0.890625}\n","   Training batch 15:\n","{'loss': 1.608476, 'accuracy': 0.921875}\n","   Training batch 16:\n","{'loss': 1.4785987, 'accuracy': 0.9166667}\n","   Training batch 17:\n","{'loss': 1.3463602, 'accuracy': 0.921875}\n","   Training batch 18:\n","{'loss': 1.7094059, 'accuracy': 0.9010417}\n","   Training batch 19:\n","{'loss': 1.3779756, 'accuracy': 0.9375}\n","   Training batch 20:\n","{'loss': 1.8980795, 'accuracy': 0.9114583}\n","   Training batch 21:\n","{'loss': 1.3813543, 'accuracy': 0.9322917}\n","   Training batch 22:\n","{'loss': 1.5936749, 'accuracy': 0.9114583}\n","   Training batch 23:\n","{'loss': 1.5044563, 'accuracy': 0.9166667}\n","   Training batch 24:\n","{'loss': 1.8088341, 'accuracy': 0.8958333}\n","   Training batch 25:\n","{'loss': 1.3315139, 'accuracy': 0.9270833}\n","   Training batch 26:\n","{'loss': 1.6809736, 'accuracy': 0.90625}\n","   Training batch 27:\n","{'loss': 1.7486613, 'accuracy': 0.90625}\n","   Training batch 28:\n","{'loss': 1.623955, 'accuracy': 0.9166667}\n","   Training batch 29:\n","{'loss': 1.3077443, 'accuracy': 0.921875}\n","   Test batch 0:\n","{'accuracy': 0.9114583}\n","   Test batch 1:\n","{'accuracy': 0.9322917}\n","   Test batch 2:\n","{'accuracy': 0.9270833}\n","Epoch 9......\n","   Training batch 0:\n","{'loss': 1.1196887, 'accuracy': 0.953125}\n","   Training batch 1:\n","{'loss': 1.995977, 'accuracy': 0.8958333}\n","   Training batch 2:\n","{'loss': 1.3820193, 'accuracy': 0.9322917}\n","   Training batch 3:\n","{'loss': 1.4261585, 'accuracy': 0.9322917}\n","   Training batch 4:\n","{'loss': 1.7692271, 'accuracy': 0.90625}\n","   Training batch 5:\n","{'loss': 1.3369926, 'accuracy': 0.9270833}\n","   Training batch 6:\n","{'loss': 1.3821603, 'accuracy': 0.9375}\n","   Training batch 7:\n","{'loss': 1.9615138, 'accuracy': 0.875}\n","   Training batch 8:\n","{'loss': 1.1500607, 'accuracy': 0.9375}\n","   Training batch 9:\n","{'loss': 1.3368703, 'accuracy': 0.9322917}\n","   Training batch 10:\n","{'loss': 1.5287088, 'accuracy': 0.9114583}\n","   Training batch 11:\n","{'loss': 1.5838435, 'accuracy': 0.9114583}\n","   Training batch 12:\n","{'loss': 1.6436143, 'accuracy': 0.90625}\n","   Training batch 13:\n","{'loss': 1.4706745, 'accuracy': 0.9270833}\n","   Training batch 14:\n","{'loss': 1.9792976, 'accuracy': 0.8854167}\n","   Training batch 15:\n","{'loss': 1.5713289, 'accuracy': 0.9270833}\n","   Training batch 16:\n","{'loss': 1.3750134, 'accuracy': 0.921875}\n","   Training batch 17:\n","{'loss': 1.3155174, 'accuracy': 0.9322917}\n","   Training batch 18:\n","{'loss': 1.7343035, 'accuracy': 0.890625}\n","   Training batch 19:\n","{'loss': 1.2810597, 'accuracy': 0.9375}\n","   Training batch 20:\n","{'loss': 1.9196565, 'accuracy': 0.9010417}\n","   Training batch 21:\n","{'loss': 1.37223, 'accuracy': 0.9375}\n","   Training batch 22:\n","{'loss': 1.6031213, 'accuracy': 0.9114583}\n","   Training batch 23:\n","{'loss': 1.5142695, 'accuracy': 0.921875}\n","   Training batch 24:\n","{'loss': 1.8131692, 'accuracy': 0.8958333}\n","   Training batch 25:\n","{'loss': 1.3387818, 'accuracy': 0.921875}\n","   Training batch 26:\n","{'loss': 1.6655188, 'accuracy': 0.90625}\n","   Training batch 27:\n","{'loss': 1.7463582, 'accuracy': 0.9114583}\n","   Training batch 28:\n","{'loss': 1.5828807, 'accuracy': 0.921875}\n","   Training batch 29:\n","{'loss': 1.2708236, 'accuracy': 0.921875}\n","   Test batch 0:\n","{'accuracy': 0.90625}\n","   Test batch 1:\n","{'accuracy': 0.9270833}\n","   Test batch 2:\n","{'accuracy': 0.921875}\n","Epoch 10......\n","   Training batch 0:\n","{'loss': 1.0599048, 'accuracy': 0.953125}\n","   Training batch 1:\n","{'loss': 2.0835505, 'accuracy': 0.8854167}\n","   Training batch 2:\n","{'loss': 1.4541757, 'accuracy': 0.9270833}\n","   Training batch 3:\n","{'loss': 1.3934946, 'accuracy': 0.9322917}\n","   Training batch 4:\n","{'loss': 1.7463794, 'accuracy': 0.90625}\n","   Training batch 5:\n","{'loss': 1.3017399, 'accuracy': 0.9322917}\n","   Training batch 6:\n","{'loss': 1.3471243, 'accuracy': 0.9322917}\n","   Training batch 7:\n","{'loss': 2.2190356, 'accuracy': 0.875}\n","   Training batch 8:\n","{'loss': 1.1496544, 'accuracy': 0.9427083}\n","   Training batch 9:\n","{'loss': 1.3207221, 'accuracy': 0.9375}\n","   Training batch 10:\n","{'loss': 1.5227633, 'accuracy': 0.9166667}\n","   Training batch 11:\n","{'loss': 1.5603653, 'accuracy': 0.9114583}\n","   Training batch 12:\n","{'loss': 1.5765693, 'accuracy': 0.9114583}\n","   Training batch 13:\n","{'loss': 1.4449782, 'accuracy': 0.9270833}\n","   Training batch 14:\n","{'loss': 1.9252318, 'accuracy': 0.890625}\n","   Training batch 15:\n","{'loss': 1.529825, 'accuracy': 0.9166667}\n","   Training batch 16:\n","{'loss': 1.3610905, 'accuracy': 0.921875}\n","   Training batch 17:\n","{'loss': 1.3866539, 'accuracy': 0.921875}\n","   Training batch 18:\n","{'loss': 1.6423289, 'accuracy': 0.9166667}\n","   Training batch 19:\n","{'loss': 1.3006159, 'accuracy': 0.9375}\n","   Training batch 20:\n","{'loss': 1.8422718, 'accuracy': 0.9114583}\n","   Training batch 21:\n","{'loss': 1.3327268, 'accuracy': 0.9375}\n","   Training batch 22:\n","{'loss': 1.5608199, 'accuracy': 0.9114583}\n","   Training batch 23:\n","{'loss': 1.5175127, 'accuracy': 0.9166667}\n","   Training batch 24:\n","{'loss': 1.8250821, 'accuracy': 0.8958333}\n","   Training batch 25:\n","{'loss': 1.3213431, 'accuracy': 0.9166667}\n","   Training batch 26:\n","{'loss': 1.63683, 'accuracy': 0.90625}\n","   Training batch 27:\n","{'loss': 1.7536992, 'accuracy': 0.8958333}\n","   Training batch 28:\n","{'loss': 1.5819378, 'accuracy': 0.921875}\n","   Training batch 29:\n","{'loss': 1.2342899, 'accuracy': 0.9166667}\n","   Test batch 0:\n","{'accuracy': 0.9114583}\n","   Test batch 1:\n","{'accuracy': 0.9375}\n","   Test batch 2:\n","{'accuracy': 0.9322917}\n","Epoch 11......\n","   Training batch 0:\n","{'loss': 1.0396113, 'accuracy': 0.9635417}\n","   Training batch 1:\n","{'loss': 1.8963218, 'accuracy': 0.90625}\n","   Training batch 2:\n","{'loss': 1.3925335, 'accuracy': 0.9322917}\n","   Training batch 3:\n","{'loss': 1.5624783, 'accuracy': 0.9166667}\n","   Training batch 4:\n","{'loss': 1.7542437, 'accuracy': 0.9114583}\n","   Training batch 5:\n","{'loss': 1.3307518, 'accuracy': 0.921875}\n","   Training batch 6:\n","{'loss': 1.5623906, 'accuracy': 0.90625}\n","   Training batch 7:\n","{'loss': 1.9296042, 'accuracy': 0.875}\n","   Training batch 8:\n","{'loss': 1.088706, 'accuracy': 0.9427083}\n","   Training batch 9:\n","{'loss': 1.3363413, 'accuracy': 0.9322917}\n","   Training batch 10:\n","{'loss': 1.5432441, 'accuracy': 0.9114583}\n","   Training batch 11:\n","{'loss': 1.5278168, 'accuracy': 0.9114583}\n","   Training batch 12:\n","{'loss': 1.5275654, 'accuracy': 0.9114583}\n","   Training batch 13:\n","{'loss': 1.4403896, 'accuracy': 0.9270833}\n","   Training batch 14:\n","{'loss': 1.8966007, 'accuracy': 0.890625}\n","   Training batch 15:\n","{'loss': 1.5070981, 'accuracy': 0.9270833}\n","   Training batch 16:\n","{'loss': 1.3416872, 'accuracy': 0.9270833}\n","   Training batch 17:\n","{'loss': 1.3247039, 'accuracy': 0.9322917}\n","   Training batch 18:\n","{'loss': 1.6618193, 'accuracy': 0.9166667}\n","   Training batch 19:\n","{'loss': 1.2664435, 'accuracy': 0.9427083}\n","   Training batch 20:\n","{'loss': 1.8579284, 'accuracy': 0.9166667}\n","   Training batch 21:\n","{'loss': 1.3126742, 'accuracy': 0.9479167}\n","   Training batch 22:\n","{'loss': 1.5573235, 'accuracy': 0.9166667}\n","   Training batch 23:\n","{'loss': 1.4619694, 'accuracy': 0.921875}\n","   Training batch 24:\n","{'loss': 1.8860086, 'accuracy': 0.8958333}\n","   Training batch 25:\n","{'loss': 1.2783272, 'accuracy': 0.9375}\n","   Training batch 26:\n","{'loss': 1.5420089, 'accuracy': 0.9114583}\n","   Training batch 27:\n","{'loss': 1.7275102, 'accuracy': 0.9010417}\n","   Training batch 28:\n","{'loss': 1.5327153, 'accuracy': 0.9166667}\n","   Training batch 29:\n","{'loss': 1.1593206, 'accuracy': 0.9322917}\n","   Test batch 0:\n","{'accuracy': 0.90625}\n","   Test batch 1:\n","{'accuracy': 0.9375}\n","   Test batch 2:\n","{'accuracy': 0.921875}\n","Epoch 12......\n","   Training batch 0:\n","{'loss': 0.9697888, 'accuracy': 0.9635417}\n","   Training batch 1:\n","{'loss': 1.5768769, 'accuracy': 0.921875}\n","   Training batch 2:\n","{'loss': 1.4027295, 'accuracy': 0.9375}\n","   Training batch 3:\n","{'loss': 1.3833015, 'accuracy': 0.9270833}\n","   Training batch 4:\n","{'loss': 1.7226105, 'accuracy': 0.9114583}\n","   Training batch 5:\n","{'loss': 1.2845834, 'accuracy': 0.9270833}\n","   Training batch 6:\n","{'loss': 1.372736, 'accuracy': 0.9322917}\n","   Training batch 7:\n","{'loss': 2.3526618, 'accuracy': 0.875}\n","   Training batch 8:\n","{'loss': 1.1423478, 'accuracy': 0.9427083}\n","   Training batch 9:\n","{'loss': 1.292781, 'accuracy': 0.9322917}\n","   Training batch 10:\n","{'loss': 1.5048416, 'accuracy': 0.9114583}\n","   Training batch 11:\n","{'loss': 1.6203012, 'accuracy': 0.9166667}\n","   Training batch 12:\n","{'loss': 1.6674587, 'accuracy': 0.90625}\n","   Training batch 13:\n","{'loss': 1.4407363, 'accuracy': 0.9114583}\n","   Training batch 14:\n","{'loss': 1.9300361, 'accuracy': 0.8854167}\n","   Training batch 15:\n","{'loss': 1.4734752, 'accuracy': 0.9322917}\n","   Training batch 16:\n","{'loss': 1.303501, 'accuracy': 0.9270833}\n","   Training batch 17:\n","{'loss': 1.2728604, 'accuracy': 0.9375}\n","   Training batch 18:\n","{'loss': 1.7548671, 'accuracy': 0.9010417}\n","   Training batch 19:\n","{'loss': 1.2156307, 'accuracy': 0.953125}\n","   Training batch 20:\n","{'loss': 1.934206, 'accuracy': 0.8958333}\n","   Training batch 21:\n","{'loss': 1.3495209, 'accuracy': 0.9270833}\n","   Training batch 22:\n","{'loss': 1.5160637, 'accuracy': 0.9114583}\n","   Training batch 23:\n","{'loss': 1.599902, 'accuracy': 0.9010417}\n","   Training batch 24:\n","{'loss': 1.7693162, 'accuracy': 0.8958333}\n","   Training batch 25:\n","{'loss': 1.2910444, 'accuracy': 0.9270833}\n","   Training batch 26:\n","{'loss': 1.5586982, 'accuracy': 0.9114583}\n","   Training batch 27:\n","{'loss': 1.8082411, 'accuracy': 0.8958333}\n","   Training batch 28:\n","{'loss': 1.5039015, 'accuracy': 0.921875}\n","   Training batch 29:\n","{'loss': 1.1329254, 'accuracy': 0.9375}\n","   Test batch 0:\n","{'accuracy': 0.9114583}\n","   Test batch 1:\n","{'accuracy': 0.921875}\n","   Test batch 2:\n","{'accuracy': 0.9114583}\n","Epoch 13......\n","   Training batch 0:\n","{'loss': 0.9608564, 'accuracy': 0.9635417}\n","   Training batch 1:\n","{'loss': 1.4826937, 'accuracy': 0.9270833}\n","   Training batch 2:\n","{'loss': 1.5334324, 'accuracy': 0.9322917}\n","   Training batch 3:\n","{'loss': 1.29357, 'accuracy': 0.9375}\n","   Training batch 4:\n","{'loss': 1.7007791, 'accuracy': 0.9114583}\n","   Training batch 5:\n","{'loss': 1.2343638, 'accuracy': 0.9322917}\n","   Training batch 6:\n","{'loss': 1.3386278, 'accuracy': 0.9322917}\n","   Training batch 7:\n","{'loss': 1.9127047, 'accuracy': 0.8854167}\n","   Training batch 8:\n","{'loss': 1.0733668, 'accuracy': 0.9375}\n","   Training batch 9:\n","{'loss': 1.239249, 'accuracy': 0.9375}\n","   Training batch 10:\n","{'loss': 1.5047866, 'accuracy': 0.9114583}\n","   Training batch 11:\n","{'loss': 1.5640017, 'accuracy': 0.921875}\n","   Training batch 12:\n","{'loss': 1.6609256, 'accuracy': 0.9166667}\n","   Training batch 13:\n","{'loss': 1.4048986, 'accuracy': 0.9166667}\n","   Training batch 14:\n","{'loss': 1.924434, 'accuracy': 0.8854167}\n","   Training batch 15:\n","{'loss': 1.4031187, 'accuracy': 0.9322917}\n","   Training batch 16:\n","{'loss': 1.3407489, 'accuracy': 0.9270833}\n","   Training batch 17:\n","{'loss': 1.3761709, 'accuracy': 0.9322917}\n","   Training batch 18:\n","{'loss': 1.7114404, 'accuracy': 0.90625}\n","   Training batch 19:\n","{'loss': 1.1667559, 'accuracy': 0.953125}\n","   Training batch 20:\n","{'loss': 1.8863302, 'accuracy': 0.90625}\n","   Training batch 21:\n","{'loss': 1.4990404, 'accuracy': 0.9114583}\n","   Training batch 22:\n","{'loss': 1.4418286, 'accuracy': 0.9270833}\n","   Training batch 23:\n","{'loss': 1.7430718, 'accuracy': 0.9010417}\n","   Training batch 24:\n","{'loss': 1.7144996, 'accuracy': 0.90625}\n","   Training batch 25:\n","{'loss': 1.3571768, 'accuracy': 0.9114583}\n","   Training batch 26:\n","{'loss': 1.5685678, 'accuracy': 0.9114583}\n","   Training batch 27:\n","{'loss': 1.6452318, 'accuracy': 0.921875}\n","   Training batch 28:\n","{'loss': 1.5238011, 'accuracy': 0.9114583}\n","   Training batch 29:\n","{'loss': 1.0799162, 'accuracy': 0.9375}\n","   Test batch 0:\n","{'accuracy': 0.90625}\n","   Test batch 1:\n","{'accuracy': 0.9270833}\n","   Test batch 2:\n","{'accuracy': 0.9114583}\n","Epoch 14......\n","   Training batch 0:\n","{'loss': 0.91279614, 'accuracy': 0.9635417}\n","   Training batch 1:\n","{'loss': 1.489028, 'accuracy': 0.921875}\n","   Training batch 2:\n","{'loss': 1.5672945, 'accuracy': 0.9322917}\n","   Training batch 3:\n","{'loss': 1.2281864, 'accuracy': 0.9375}\n","   Training batch 4:\n","{'loss': 1.7352424, 'accuracy': 0.90625}\n","   Training batch 5:\n","{'loss': 1.2587671, 'accuracy': 0.9270833}\n","   Training batch 6:\n","{'loss': 1.294359, 'accuracy': 0.9322917}\n","   Training batch 7:\n","{'loss': 1.7923654, 'accuracy': 0.8958333}\n","   Training batch 8:\n","{'loss': 1.0483495, 'accuracy': 0.9375}\n","   Training batch 9:\n","{'loss': 1.2459929, 'accuracy': 0.9322917}\n","   Training batch 10:\n","{'loss': 1.4205081, 'accuracy': 0.9166667}\n","   Training batch 11:\n","{'loss': 1.5612273, 'accuracy': 0.9166667}\n","   Training batch 12:\n","{'loss': 1.5863652, 'accuracy': 0.9166667}\n","   Training batch 13:\n","{'loss': 1.4188757, 'accuracy': 0.9114583}\n","   Training batch 14:\n","{'loss': 1.904715, 'accuracy': 0.8854167}\n","   Training batch 15:\n","{'loss': 1.4183762, 'accuracy': 0.921875}\n","   Training batch 16:\n","{'loss': 1.3683898, 'accuracy': 0.9166667}\n","   Training batch 17:\n","{'loss': 1.2383983, 'accuracy': 0.9375}\n","   Training batch 18:\n","{'loss': 1.6664345, 'accuracy': 0.921875}\n","   Training batch 19:\n","{'loss': 1.2965145, 'accuracy': 0.9375}\n","   Training batch 20:\n","{'loss': 1.7970659, 'accuracy': 0.9114583}\n","   Training batch 21:\n","{'loss': 1.2568978, 'accuracy': 0.9322917}\n","   Training batch 22:\n","{'loss': 1.5372834, 'accuracy': 0.921875}\n","   Training batch 23:\n","{'loss': 1.4747858, 'accuracy': 0.9166667}\n","   Training batch 24:\n","{'loss': 1.7850016, 'accuracy': 0.8958333}\n","   Training batch 25:\n","{'loss': 1.2098315, 'accuracy': 0.9322917}\n","   Training batch 26:\n","{'loss': 1.4966661, 'accuracy': 0.90625}\n","   Training batch 27:\n","{'loss': 1.6339418, 'accuracy': 0.921875}\n","   Training batch 28:\n","{'loss': 1.4897854, 'accuracy': 0.9166667}\n","   Training batch 29:\n","{'loss': 1.0529829, 'accuracy': 0.9479167}\n","   Test batch 0:\n","{'accuracy': 0.9114583}\n","   Test batch 1:\n","{'accuracy': 0.9375}\n","   Test batch 2:\n","{'accuracy': 0.9114583}\n","Epoch 15......\n","   Training batch 0:\n","{'loss': 0.8679908, 'accuracy': 0.9635417}\n","   Training batch 1:\n","{'loss': 1.4551454, 'accuracy': 0.9322917}\n","   Training batch 2:\n","{'loss': 1.5230956, 'accuracy': 0.9322917}\n","   Training batch 3:\n","{'loss': 1.2218246, 'accuracy': 0.9375}\n","   Training batch 4:\n","{'loss': 1.6889637, 'accuracy': 0.9114583}\n","   Training batch 5:\n","{'loss': 1.2099166, 'accuracy': 0.9270833}\n","   Training batch 6:\n","{'loss': 1.2725102, 'accuracy': 0.9322917}\n","   Training batch 7:\n","{'loss': 2.140327, 'accuracy': 0.875}\n","   Training batch 8:\n","{'loss': 1.0415865, 'accuracy': 0.9479167}\n","   Training batch 9:\n","{'loss': 1.2141482, 'accuracy': 0.9322917}\n","   Training batch 10:\n","{'loss': 1.4286797, 'accuracy': 0.9166667}\n","   Training batch 11:\n","{'loss': 1.5236942, 'accuracy': 0.9166667}\n","   Training batch 12:\n","{'loss': 1.6353042, 'accuracy': 0.9166667}\n","   Training batch 13:\n","{'loss': 1.4314535, 'accuracy': 0.921875}\n","   Training batch 14:\n","{'loss': 1.9101017, 'accuracy': 0.8958333}\n","   Training batch 15:\n","{'loss': 1.3643892, 'accuracy': 0.9270833}\n","   Training batch 16:\n","{'loss': 1.4067802, 'accuracy': 0.9166667}\n","   Training batch 17:\n","{'loss': 1.2626898, 'accuracy': 0.9375}\n","   Training batch 18:\n","{'loss': 1.6113226, 'accuracy': 0.9114583}\n","   Training batch 19:\n","{'loss': 1.1256121, 'accuracy': 0.9583333}\n","   Training batch 20:\n","{'loss': 1.7386485, 'accuracy': 0.9166667}\n","   Training batch 21:\n","{'loss': 1.3333671, 'accuracy': 0.9270833}\n","   Training batch 22:\n","{'loss': 1.5029292, 'accuracy': 0.9270833}\n","   Training batch 23:\n","{'loss': 1.4247241, 'accuracy': 0.9322917}\n","   Training batch 24:\n","{'loss': 1.6052113, 'accuracy': 0.9114583}\n","   Training batch 25:\n","{'loss': 1.2996831, 'accuracy': 0.9166667}\n","   Training batch 26:\n","{'loss': 1.4530936, 'accuracy': 0.9114583}\n","   Training batch 27:\n","{'loss': 1.6613835, 'accuracy': 0.9114583}\n","   Training batch 28:\n","{'loss': 1.4501287, 'accuracy': 0.9166667}\n","   Training batch 29:\n","{'loss': 1.0142282, 'accuracy': 0.953125}\n","   Test batch 0:\n","{'accuracy': 0.90625}\n","   Test batch 1:\n","{'accuracy': 0.921875}\n","   Test batch 2:\n","{'accuracy': 0.9166667}\n","Epoch 16......\n","   Training batch 0:\n","{'loss': 0.80907387, 'accuracy': 0.9739583}\n","   Training batch 1:\n","{'loss': 2.3305879, 'accuracy': 0.8854167}\n","   Training batch 2:\n","{'loss': 1.186256, 'accuracy': 0.9479167}\n","   Training batch 3:\n","{'loss': 1.299969, 'accuracy': 0.953125}\n","   Training batch 4:\n","{'loss': 1.8337696, 'accuracy': 0.9166667}\n","   Training batch 5:\n","{'loss': 1.2885463, 'accuracy': 0.921875}\n","   Training batch 6:\n","{'loss': 1.2367938, 'accuracy': 0.9427083}\n","   Training batch 7:\n","{'loss': 2.285328, 'accuracy': 0.8854167}\n","   Training batch 8:\n","{'loss': 1.0895078, 'accuracy': 0.953125}\n","   Training batch 9:\n","{'loss': 1.1992549, 'accuracy': 0.9375}\n","   Training batch 10:\n","{'loss': 1.3635908, 'accuracy': 0.921875}\n","   Training batch 11:\n","{'loss': 1.4991245, 'accuracy': 0.921875}\n","   Training batch 12:\n","{'loss': 1.6344526, 'accuracy': 0.9166667}\n","   Training batch 13:\n","{'loss': 1.3529534, 'accuracy': 0.9166667}\n","   Training batch 14:\n","{'loss': 1.8609526, 'accuracy': 0.8854167}\n","   Training batch 15:\n","{'loss': 1.316618, 'accuracy': 0.9322917}\n","   Training batch 16:\n","{'loss': 1.3328587, 'accuracy': 0.921875}\n","   Training batch 17:\n","{'loss': 1.3382099, 'accuracy': 0.9375}\n","   Training batch 18:\n","{'loss': 1.702766, 'accuracy': 0.890625}\n","   Training batch 19:\n","{'loss': 1.2492158, 'accuracy': 0.953125}\n","   Training batch 20:\n","{'loss': 1.6498404, 'accuracy': 0.9114583}\n","   Training batch 21:\n","{'loss': 1.26032, 'accuracy': 0.9375}\n","   Training batch 22:\n","{'loss': 1.4917307, 'accuracy': 0.921875}\n","   Training batch 23:\n","{'loss': 1.4611814, 'accuracy': 0.9114583}\n","   Training batch 24:\n","{'loss': 1.7411461, 'accuracy': 0.8958333}\n","   Training batch 25:\n","{'loss': 1.2257186, 'accuracy': 0.9166667}\n","   Training batch 26:\n","{'loss': 1.4022934, 'accuracy': 0.9114583}\n","   Training batch 27:\n","{'loss': 1.6518863, 'accuracy': 0.9114583}\n","   Training batch 28:\n","{'loss': 1.4287835, 'accuracy': 0.9166667}\n","   Training batch 29:\n","{'loss': 0.9971579, 'accuracy': 0.9583333}\n","   Test batch 0:\n","{'accuracy': 0.9114583}\n","   Test batch 1:\n","{'accuracy': 0.9322917}\n","   Test batch 2:\n","{'accuracy': 0.9166667}\n","Epoch 17......\n","   Training batch 0:\n","{'loss': 0.7962842, 'accuracy': 0.9739583}\n","   Training batch 1:\n","{'loss': 1.725775, 'accuracy': 0.9166667}\n","   Training batch 2:\n","{'loss': 1.2996333, 'accuracy': 0.9322917}\n","   Training batch 3:\n","{'loss': 1.3632581, 'accuracy': 0.9270833}\n","   Training batch 4:\n","{'loss': 1.7214597, 'accuracy': 0.9166667}\n","   Training batch 5:\n","{'loss': 1.1880995, 'accuracy': 0.9270833}\n","   Training batch 6:\n","{'loss': 1.2023518, 'accuracy': 0.9375}\n","   Training batch 7:\n","{'loss': 2.1726966, 'accuracy': 0.875}\n","   Training batch 8:\n","{'loss': 1.026089, 'accuracy': 0.953125}\n","   Training batch 9:\n","{'loss': 1.2147349, 'accuracy': 0.9375}\n","   Training batch 10:\n","{'loss': 1.3671356, 'accuracy': 0.921875}\n","   Training batch 11:\n","{'loss': 1.4449984, 'accuracy': 0.921875}\n","   Training batch 12:\n","{'loss': 1.6544508, 'accuracy': 0.9114583}\n","   Training batch 13:\n","{'loss': 1.3951724, 'accuracy': 0.921875}\n","   Training batch 14:\n","{'loss': 2.0062506, 'accuracy': 0.8854167}\n","   Training batch 15:\n","{'loss': 1.2889521, 'accuracy': 0.9322917}\n","   Training batch 16:\n","{'loss': 1.2920818, 'accuracy': 0.9322917}\n","   Training batch 17:\n","{'loss': 1.2147115, 'accuracy': 0.9427083}\n","   Training batch 18:\n","{'loss': 1.7318913, 'accuracy': 0.9270833}\n","   Training batch 19:\n","{'loss': 1.1751218, 'accuracy': 0.9427083}\n","   Training batch 20:\n","{'loss': 1.7423316, 'accuracy': 0.90625}\n","   Training batch 21:\n","{'loss': 1.237691, 'accuracy': 0.9270833}\n","   Training batch 22:\n","{'loss': 1.3413808, 'accuracy': 0.9375}\n","   Training batch 23:\n","{'loss': 1.2900665, 'accuracy': 0.9375}\n","   Training batch 24:\n","{'loss': 1.6214122, 'accuracy': 0.90625}\n","   Training batch 25:\n","{'loss': 1.2706497, 'accuracy': 0.9114583}\n","   Training batch 26:\n","{'loss': 1.3732364, 'accuracy': 0.9114583}\n","   Training batch 27:\n","{'loss': 1.6852968, 'accuracy': 0.9114583}\n","   Training batch 28:\n","{'loss': 1.4122363, 'accuracy': 0.90625}\n","   Training batch 29:\n","{'loss': 1.0525353, 'accuracy': 0.9375}\n","   Test batch 0:\n","{'accuracy': 0.9114583}\n","   Test batch 1:\n","{'accuracy': 0.921875}\n","   Test batch 2:\n","{'accuracy': 0.9114583}\n","Epoch 18......\n","   Training batch 0:\n","{'loss': 0.8954728, 'accuracy': 0.9583333}\n","   Training batch 1:\n","{'loss': 1.3867679, 'accuracy': 0.9375}\n","   Training batch 2:\n","{'loss': 1.3195246, 'accuracy': 0.9322917}\n","   Training batch 3:\n","{'loss': 1.2812591, 'accuracy': 0.9375}\n","   Training batch 4:\n","{'loss': 1.69381, 'accuracy': 0.90625}\n","   Training batch 5:\n","{'loss': 1.2639909, 'accuracy': 0.921875}\n","   Training batch 6:\n","{'loss': 1.3204331, 'accuracy': 0.9166667}\n","   Training batch 7:\n","{'loss': 2.0734982, 'accuracy': 0.8802083}\n","   Training batch 8:\n","{'loss': 0.9818291, 'accuracy': 0.9479167}\n","   Training batch 9:\n","{'loss': 1.1503811, 'accuracy': 0.9375}\n","   Training batch 10:\n","{'loss': 1.3159599, 'accuracy': 0.9166667}\n","   Training batch 11:\n","{'loss': 1.5105783, 'accuracy': 0.9114583}\n","   Training batch 12:\n","{'loss': 1.4835144, 'accuracy': 0.9270833}\n","   Training batch 13:\n","{'loss': 1.2873826, 'accuracy': 0.9166667}\n","   Training batch 14:\n","{'loss': 2.0566163, 'accuracy': 0.8958333}\n","   Training batch 15:\n","{'loss': 1.2902365, 'accuracy': 0.9375}\n","   Training batch 16:\n","{'loss': 1.2647513, 'accuracy': 0.9270833}\n","   Training batch 17:\n","{'loss': 1.3153862, 'accuracy': 0.9322917}\n","   Training batch 18:\n","{'loss': 1.7170091, 'accuracy': 0.8958333}\n","   Training batch 19:\n","{'loss': 1.107327, 'accuracy': 0.953125}\n","   Training batch 20:\n","{'loss': 1.6580726, 'accuracy': 0.9010417}\n","   Training batch 21:\n","{'loss': 1.2341101, 'accuracy': 0.9375}\n","   Training batch 22:\n","{'loss': 1.40455, 'accuracy': 0.921875}\n","   Training batch 23:\n","{'loss': 1.4363363, 'accuracy': 0.9166667}\n","   Training batch 24:\n","{'loss': 1.6269567, 'accuracy': 0.90625}\n","   Training batch 25:\n","{'loss': 1.1818751, 'accuracy': 0.9166667}\n","   Training batch 26:\n","{'loss': 1.2937181, 'accuracy': 0.9166667}\n","   Training batch 27:\n","{'loss': 1.6479759, 'accuracy': 0.9114583}\n","   Training batch 28:\n","{'loss': 1.332428, 'accuracy': 0.9166667}\n","   Training batch 29:\n","{'loss': 1.1036117, 'accuracy': 0.9322917}\n","   Test batch 0:\n","{'accuracy': 0.90625}\n","   Test batch 1:\n","{'accuracy': 0.9322917}\n","   Test batch 2:\n","{'accuracy': 0.921875}\n","Epoch 19......\n","   Training batch 0:\n","{'loss': 0.9619696, 'accuracy': 0.9583333}\n","   Training batch 1:\n","{'loss': 2.024587, 'accuracy': 0.8854167}\n","   Training batch 2:\n","{'loss': 1.2159661, 'accuracy': 0.953125}\n","   Training batch 3:\n","{'loss': 1.329437, 'accuracy': 0.9427083}\n","   Training batch 4:\n","{'loss': 1.6164596, 'accuracy': 0.9114583}\n","   Training batch 5:\n","{'loss': 1.3142625, 'accuracy': 0.9114583}\n","   Training batch 6:\n","{'loss': 1.4738524, 'accuracy': 0.90625}\n","   Training batch 7:\n","{'loss': 1.9059598, 'accuracy': 0.8854167}\n","   Training batch 8:\n","{'loss': 1.0561544, 'accuracy': 0.9375}\n","   Training batch 9:\n","{'loss': 1.1702753, 'accuracy': 0.9427083}\n","   Training batch 10:\n","{'loss': 1.4758286, 'accuracy': 0.9270833}\n","   Training batch 11:\n","{'loss': 1.4989903, 'accuracy': 0.9166667}\n","   Training batch 12:\n","{'loss': 1.448087, 'accuracy': 0.9166667}\n","   Training batch 13:\n","{'loss': 1.3237448, 'accuracy': 0.9166667}\n","   Training batch 14:\n","{'loss': 1.7214346, 'accuracy': 0.9010417}\n","   Training batch 15:\n","{'loss': 1.3175405, 'accuracy': 0.9322917}\n","   Training batch 16:\n","{'loss': 1.2215903, 'accuracy': 0.9322917}\n","   Training batch 17:\n","{'loss': 1.2083905, 'accuracy': 0.9375}\n","   Training batch 18:\n","{'loss': 1.6396639, 'accuracy': 0.9375}\n","   Training batch 19:\n","{'loss': 1.1408807, 'accuracy': 0.9479167}\n","   Training batch 20:\n","{'loss': 1.611122, 'accuracy': 0.9166667}\n","   Training batch 21:\n","{'loss': 1.1620513, 'accuracy': 0.9479167}\n","   Training batch 22:\n","{'loss': 1.3371385, 'accuracy': 0.921875}\n","   Training batch 23:\n","{'loss': 1.3235085, 'accuracy': 0.9270833}\n","   Training batch 24:\n","{'loss': 1.5650501, 'accuracy': 0.90625}\n","   Training batch 25:\n","{'loss': 1.1774706, 'accuracy': 0.9166667}\n","   Training batch 26:\n","{'loss': 1.3015969, 'accuracy': 0.921875}\n","   Training batch 27:\n","{'loss': 1.6542968, 'accuracy': 0.9114583}\n","   Training batch 28:\n","{'loss': 1.3107713, 'accuracy': 0.9166667}\n","   Training batch 29:\n","{'loss': 1.118081, 'accuracy': 0.9322917}\n","   Test batch 0:\n","{'accuracy': 0.9114583}\n","   Test batch 1:\n","{'accuracy': 0.921875}\n","   Test batch 2:\n","{'accuracy': 0.9322917}\n"]}]},{"cell_type":"code","source":["checkpoint_path = \"training_1/cp.ckpt\"\n","checkpoint_dir = os.path.dirname(checkpoint_path)\n","\n","# Create a callback that saves the model's weights\n","# cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n","#                                                  save_weights_only=True,\n","#                                                  verbose=1)"],"metadata":{"id":"Sqbqyws_9zB2","executionInfo":{"status":"ok","timestamp":1659434020398,"user_tz":-480,"elapsed":409,"user":{"displayName":"Jie Zhang","userId":"17233986917752820527"}}},"execution_count":199,"outputs":[]},{"cell_type":"code","source":["checkpoint = tf.keras.callbacks.ModelCheckpoint(checkpoint_path, \n","                             monitor='accuracy',\n","                             verbose=1,\n","                             save_best_only=True, \n","                             save_weights_only=True, \n","                             mode='max')"],"metadata":{"id":"_lDUHE5N-Vk2","executionInfo":{"status":"ok","timestamp":1659434037929,"user_tz":-480,"elapsed":398,"user":{"displayName":"Jie Zhang","userId":"17233986917752820527"}}},"execution_count":201,"outputs":[]},{"cell_type":"code","source":["os.listdir(checkpoint_dir)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":169},"id":"lp-_nDOP-3dt","executionInfo":{"status":"error","timestamp":1659434139497,"user_tz":-480,"elapsed":415,"user":{"displayName":"Jie Zhang","userId":"17233986917752820527"}},"outputId":"c9c275d4-86bc-4348-876f-d7867ef9a501"},"execution_count":203,"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-203-d5cf370e114f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'training_1'"]}]},{"cell_type":"code","source":["checkpoint"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gI9TGuAg-gev","executionInfo":{"status":"ok","timestamp":1659434045179,"user_tz":-480,"elapsed":385,"user":{"displayName":"Jie Zhang","userId":"17233986917752820527"}},"outputId":"d60c1514-57e0-4e24-82d8-5ea20f703537"},"execution_count":202,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<keras.callbacks.ModelCheckpoint at 0x7f4ac72b69d0>"]},"metadata":{},"execution_count":202}]},{"cell_type":"code","source":["def main():\n","    # the path to save models\n","    save_path = './model/'\n","\n","    print('reading wordembedding')\n","    wordembedding = np.load('./data/vec.npy')\n","\n","    print('reading training data')\n","    train_y = np.load('./data/train_y.npy')\n","    train_word = np.load('./data/train_word.npy')\n","    train_pos1 = np.load('./data/train_pos1.npy')\n","    train_pos2 = np.load('./data/train_pos2.npy')\n","\n","    vocab_size = len(wordembedding)\n","    num_classes = len(train_y[0])\n","    total_num = 50\n","\n","    def train_step(word_batch, pos1_batch, pos2_batch, y_batch, total_num):\n","      \n"],"metadata":{"id":"ITPTxS6lEpPt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"dxmj_OZUXDsW"},"execution_count":null,"outputs":[]}]}